{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ğŸ¯ ESè¨ºæ–­ãƒ„ãƒ¼ãƒ« - Google Colabèµ·å‹•ç”¨\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€ESè¨ºæ–­ãƒ„ãƒ¼ãƒ«ã‚’Google Colabä¸Šã§èµ·å‹•ã—ã¾ã™ã€‚\n\n## ğŸ“‹ å®Ÿè¡Œæ‰‹é †\n\n1. **ã‚»ãƒ«1**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n2. **ã‚»ãƒ«2**: GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n3. **ã‚»ãƒ«3**: CSVãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆGoogle Drive / ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰\n4. **ã‚»ãƒ«4**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãƒ»èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»é«˜é€ŸåŒ–ï¼‰\n5. **ã‚»ãƒ«5**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•\n6. **ã‚»ãƒ«6**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»æ¬¡å›èµ·å‹•ã‚’é«˜é€ŸåŒ–ï¼‰\n\n## âš¡ é«˜é€Ÿèµ·å‹•ã«ã¤ã„ã¦\n\n**åˆå›èµ·å‹•**: ã‚»ãƒ«1ã€œ5ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆ3-5åˆ†ï¼‰\n**2å›ç›®ä»¥é™**: ã‚»ãƒ«6ã§ä¿å­˜ã—ãŸå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ•°ç§’ã§èµ·å‹•ï¼\n\n---",
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ã‚»ãƒ«1: å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ],
   "metadata": {
    "id": "install"
   }
  },
  {
   "cell_type": "code",
   "source": "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n!pip install flask flask-cors pandas numpy scikit-learn tqdm -q\n\n# sentence-transformers ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ï¼‰\nprint(\"ğŸ”§ sentence-transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n!pip install sentence-transformers -q\nprint(\"âœ… sentence-transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n\n# pyngrokã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰\n!pip install pyngrok -q --default-timeout=100\n\nprint(\"âœ… å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")\nprint(\"\\nğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:\")\nprint(\"  - Flask (Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)\")\nprint(\"  - pandas (ãƒ‡ãƒ¼ã‚¿å‡¦ç†)\")\nprint(\"  - numpy (æ•°å€¤è¨ˆç®—)\")\nprint(\"  - scikit-learn (TF-IDFã€æ©Ÿæ¢°å­¦ç¿’)\")\nprint(\"  - sentence-transformers (ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨BERT)\")\nprint(\"  - tqdm (é€²æ—è¡¨ç¤º)\")\nprint(\"  - pyngrok (å…¬é–‹URLä½œæˆ)\")\nprint(\"\\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–:\")\nprint(\"  - ãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–ï¼ˆ40åˆ† â†’ 3-5åˆ†ï¼‰\")\nprint(\"  - GPUå¯¾å¿œï¼ˆT4æœ‰åŠ¹æ™‚ã¯ã•ã‚‰ã«é«˜é€Ÿï¼‰\")",
   "metadata": {
    "id": "cell1",
    "outputId": "20a13929-e15e-46d6-fb2a-f4ddb1309db8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ã‚»ãƒ«2: GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³"
   ],
   "metadata": {
    "id": "clone"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/ryosuke-fujii/es.git\n",
    "\n",
    "# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´\n",
    "%cd es\n",
    "\n",
    "print(\"âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(\"\\nğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ :\")\n",
    "!ls -la"
   ],
   "metadata": {
    "id": "cell2",
    "outputId": "9dc64cb3-3685-47c6-daea-bcea272b32c7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'es'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 27 (delta 6), reused 26 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (27/27), 21.09 KiB | 2.64 MiB/s, done.\n",
      "Resolving deltas: 100% (6/6), done.\n",
      "/content/es\n",
      "âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸ\n",
      "\n",
      "ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ :\n",
      "total 56\n",
      "drwxr-xr-x 7 root root  4096 Nov  4 15:37 .\n",
      "drwxr-xr-x 1 root root  4096 Nov  4 15:37 ..\n",
      "-rw-r--r-- 1 root root 10402 Nov  4 15:37 COLAB_GUIDE.md\n",
      "drwxr-xr-x 2 root root  4096 Nov  4 15:37 data\n",
      "drwxr-xr-x 8 root root  4096 Nov  4 15:37 .git\n",
      "-rw-r--r-- 1 root root   486 Nov  4 15:37 .gitignore\n",
      "drwxr-xr-x 2 root root  4096 Nov  4 15:37 notebooks\n",
      "-rw-r--r-- 1 root root  6790 Nov  4 15:37 README.md\n",
      "-rw-r--r-- 1 root root    94 Nov  4 15:37 requirements.txt\n",
      "drwxr-xr-x 2 root root  4096 Nov  4 15:37 src\n",
      "drwxr-xr-x 2 root root  4096 Nov  4 15:37 templates\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ã‚»ãƒ«3: CSVãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "\n",
    "ä»¥ä¸‹ã®2ã¤ã®æ–¹æ³•ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š\n",
    "- **æ–¹æ³•A**: Google Driveã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆæ¨å¥¨ï¼‰\n",
    "- **æ–¹æ³•B**: ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
   ],
   "metadata": {
    "id": "data"
   }
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# æ–¹æ³•A: Google Driveã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆæ¨å¥¨ï¼‰\n# ============================================\n\nfrom google.colab import drive\nimport os\n\n# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\ndrive.mount('/content/drive')\n\n# çµ±åˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆå°±æ´»ä¼šè­° + ãƒ¯ãƒ³ã‚­ãƒ£ãƒªã‚¢ï¼‰\ncsv_path = \"/content/drive/MyDrive/ä¼ç”»ãƒ»ãƒãƒ¼ã‚±ãƒãƒ¼ãƒ /gaxi_è‡ªå‹•åŒ–/Python/ESãƒ‡ãƒ¼ã‚¿æ•´å½¢/unified_es_data.csv\"\n\n# ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\nif os.path.exists(csv_path):\n    print(f\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {csv_path}\")\n    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ç¢ºèª\n    import pandas as pd\n    df = pd.read_csv(csv_path)\n    print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶\")\n    if 'data_source' in df.columns:\n        print(\"\\nãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†…è¨³:\")\n        print(df['data_source'].value_counts())\nelse:\n    print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}\")\n    print(\"\\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ã§ç›®çš„ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ 'ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼'\")",
   "metadata": {
    "id": "cell3a",
    "outputId": "db4108da-2a08-43e0-cc2c-ff2a98b41911",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# æ–¹æ³•B: ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "# ============================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "csv_path = list(uploaded.keys())[0]\n",
    "print(f\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {csv_path}\")"
   ],
   "metadata": {
    "id": "cell3b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ã‚»ãƒ«5: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•",
   "metadata": {
    "id": "run"
   }
  },
  {
   "cell_type": "code",
   "source": "import threading\nimport time\nimport pickle\nimport numpy as np\nfrom scipy import sparse\nimport sys\nimport os\n\n# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆé‡è¦ï¼ï¼‰\nsys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n\n# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nfrom app import app, load_csv_data, es_data, vectorizer, tfidf_matrix, sentence_model\n\n# ============================================\n# ngrokã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\n# ============================================\n# https://dashboard.ngrok.com/ ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¦ãã ã•ã„\n\nNGROK_TOKEN = \"YOUR_NGROK_TOKEN\"  # ã“ã“ã«ã‚ãªãŸã®ngrokãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›\n\n# ngrokã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰\nprint(\"ğŸ”§ ngrokã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\")\nfor attempt in range(3):\n    try:\n        from pyngrok import ngrok\n        ngrok.set_auth_token(NGROK_TOKEN)\n        print(\"âœ… ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã—ãŸ\")\n        break\n    except Exception as e:\n        if attempt < 2:\n            print(f\"âš ï¸ ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/3...\")\n            time.sleep(5)\n        else:\n            print(f\"âŒ ngrokã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n            print(\"\\nğŸ’¡ è§£æ±ºæ–¹æ³•:\")\n            print(\"1. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã‚‚ã†ä¸€åº¦è©¦ã™\")\n            print(\"2. ã‚»ãƒ«1ã‚’å†å®Ÿè¡Œã—ã¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n            raise\n\n# ============================================\n# ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æ–°è¦ï¼‰\n# ============================================\n\n# USE_PREPROCESSEDãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š\nif 'USE_PREPROCESSED' not in globals():\n    USE_PREPROCESSED = False\n    preprocessed_files = {}\n    preprocessed_dir = \"\"\n    print(\"â„¹ï¸ ã‚»ãƒ«4ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãŸã‚ã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“\")\n    print(\"â³ æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ3-5åˆ†ç¨‹åº¦ï¼‰\\n\")\n\nif USE_PREPROCESSED:\n    print(\"\\nâš¡ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n    print(\"ğŸ’¡ ã“ã‚Œã«ã‚ˆã‚Šèµ·å‹•ãŒå¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¾ã™\")\n    \n    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«appãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å–å¾—\n    import app as app_module\n    \n    # es_dataã‚’èª­ã¿è¾¼ã¿\n    with open(preprocessed_files['es_data'], 'rb') as f:\n        app_module.es_data = pickle.load(f)\n    print(f\"  âœ… es_data: {len(app_module.es_data)}ä»¶\")\n    \n    # TF-IDFè¡Œåˆ—ã‚’èª­ã¿è¾¼ã¿\n    app_module.tfidf_matrix = sparse.load_npz(preprocessed_files['tfidf_matrix'])\n    print(f\"  âœ… tfidf_matrix: {app_module.tfidf_matrix.shape}\")\n    \n    # Vectorizerã‚’èª­ã¿è¾¼ã¿\n    with open(preprocessed_files['vectorizer'], 'rb') as f:\n        app_module.vectorizer = pickle.load(f)\n    print(f\"  âœ… vectorizer\")\n    \n    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿\n    if os.path.exists(preprocessed_files['embeddings']):\n        # Sentence-BERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—ã«ã¯ä¸è¦ã ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯å¿…è¦ï¼‰\n        try:\n            from sentence_transformers import SentenceTransformer\n            if app_module.sentence_model is None:\n                print(\"  ğŸ“¥ Sentence-BERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n                app_module.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n                print(\"  âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n        except ImportError:\n            print(\"  âš ï¸ sentence-transformersãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¯ç„¡åŠ¹ã§ã™ã€‚\")\n        \n        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿\n        app_module.es_data['semantic_embedding'] = list(np.load(preprocessed_files['embeddings']))\n        print(f\"  âœ… semantic_embeddings: {len(app_module.es_data['semantic_embedding'])}ä»¶\")\n    \n    print(\"\\nâœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆæ•°ç§’ã§å®Œäº†ï¼‰\")\n    \nelse:\n    print(\"\\nğŸ“‚ CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n    print(\"â³ åˆå›å®Ÿè¡Œæ™‚ã¯Sentence-BERTãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ•°åˆ†ã‹ã‹ã‚Šã¾ã™...\")\n    print(\"âš¡ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã¯3-5åˆ†ç¨‹åº¦ã§å®Œäº†ã—ã¾ã™ï¼ˆãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–æ¸ˆã¿ï¼‰\")\n    print(\"\")\n    load_csv_data(csv_path)\n    print(\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n\n# ============================================\n# Flaskã‚¢ãƒ—ãƒªã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n# ============================================\ndef run_flask():\n    app.run(port=5000)\n\nflask_thread = threading.Thread(target=run_flask, daemon=True)\nflask_thread.start()\n\nprint(\"\\nğŸš€ Flaskã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã—ãŸ (ãƒãƒ¼ãƒˆ5000)\")\n\n# ============================================\n# ngrokãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆ\n# ============================================\ntime.sleep(2)  # Flaskã®èµ·å‹•ã‚’å¾…ã¤\n\nprint(\"\\nğŸŒ ngrokãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆä¸­...\")\ntry:\n    public_url = ngrok.connect(5000)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ‰ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ã¾ã—ãŸï¼\")\n    print(\"=\"*60)\n    print(f\"\\nğŸŒ å…¬é–‹URL: {public_url}\")\n    print(\"\\nğŸ’¡ ä¸Šè¨˜ã®URLã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„\")\n    print(\"\\nâš ï¸ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‰ã˜ã‚‹ã¨ã‚¢ãƒ—ãƒªã‚‚åœæ­¢ã—ã¾ã™\")\n    print(\"\\nğŸ” æ­è¼‰æ©Ÿèƒ½:\")\n    print(\"  - TF-IDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°\")\n    print(\"  - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆBERTï¼‰ã§æ„å‘³çš„é¡ä¼¼æ€§ã‚’ç†è§£\")\n    print(\"  - STARæ§‹é€ åˆ†æã§ESå“è³ªã‚’è©•ä¾¡\")\n    \n    if USE_PREPROCESSED:\n        print(\"\\nâš¡ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨:\")\n        print(\"  - èµ·å‹•æ™‚é–“: æ•°ç§’ï¼ˆé€šå¸¸3-5åˆ†ã®ã¨ã“ã‚ï¼‰\")\n        print(f\"  - ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {preprocessed_dir}\")\n    else:\n        print(\"\\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:\")\n        print(\"  - ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆãŒ40åˆ†â†’3-5åˆ†ã«çŸ­ç¸®\")\n        print(\"  - è»½é‡ãƒ¢ãƒ‡ãƒ«æ¡ç”¨: 384æ¬¡å…ƒï¼ˆå¾“æ¥ã®768æ¬¡å…ƒã‹ã‚‰é«˜é€ŸåŒ–ï¼‰\")\n        print(\"  - GPUè‡ªå‹•æ¤œå‡º: T4æœ‰åŠ¹æ™‚ã¯30ç§’ã€œ1åˆ†ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ\")\n        print(f\"\\nğŸ’¾ æ¬¡å›èµ·å‹•ç”¨ã«å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã«ã¯ã€ã‚»ãƒ«6ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n    \n    print(\"=\"*60)\nexcept Exception as e:\n    print(f\"\\nâŒ ngrokãƒˆãƒ³ãƒãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n    print(\"\\nğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n    print(\"1. NGROK_TOKENãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n    print(\"2. https://dashboard.ngrok.com/ ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å†ç¢ºèª\")\n    print(\"3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™\")\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ã‚»ãƒ«4: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãƒ»èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n\nå‰å›å®Ÿè¡Œæ™‚ã«ä¿å­˜ã—ãŸå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆã—ã¾ã™ã€‚\nã“ã‚Œã«ã‚ˆã‚Š2å›ç›®ä»¥é™ã®èµ·å‹•ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ï¼ˆ3-5åˆ† â†’ æ•°ç§’ï¼‰ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport sys\n\n# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ \nsys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n\n# ============================================\n# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹è¨­å®š\n# ============================================\n# CSVã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜\ncsv_dir = os.path.dirname(csv_path)\npreprocessed_dir = os.path.join(csv_dir, 'es_preprocessed_data')\n\n# CSVãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ™ãƒ¼ã‚¹åã‚’å–å¾—\ncsv_basename = os.path.splitext(os.path.basename(csv_path))[0]\n\n# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\npreprocessed_files = {\n    'es_data': os.path.join(preprocessed_dir, f'{csv_basename}_es_data.pkl'),\n    'tfidf_matrix': os.path.join(preprocessed_dir, f'{csv_basename}_tfidf_matrix.npz'),\n    'vectorizer': os.path.join(preprocessed_dir, f'{csv_basename}_vectorizer.pkl'),\n    'embeddings': os.path.join(preprocessed_dir, f'{csv_basename}_embeddings.npy')\n}\n\n# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\nall_files_exist = all(os.path.exists(f) for f in preprocessed_files.values())\n\nif all_files_exist:\n    print(\"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ã—ã¾ã—ãŸï¼\")\n    print(f\"ğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n    print(\"\\nğŸ“ åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«:\")\n    for name, path in preprocessed_files.items():\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        print(f\"  - {name}: {size_mb:.2f} MB\")\n    print(\"\\nğŸ’¡ ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã¨èµ·å‹•ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ï¼ˆæ•°ç§’ã§å®Œäº†ï¼‰\")\n    USE_PREPROCESSED = True\nelse:\n    print(\"â„¹ï¸ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n    print(\"â³ åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ3-5åˆ†ç¨‹åº¦ï¼‰\")\n    print(f\"ğŸ’¾ å®Œäº†å¾Œã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»¥ä¸‹ã«ä¿å­˜ã—ã¾ã™:\")\n    print(f\"   {preprocessed_dir}\")\n    USE_PREPROCESSED = False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ã‚»ãƒ«6: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n\n**æ¬¡å›èµ·å‹•ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’Google Driveã«ä¿å­˜ã—ã¾ã™ã€‚**\n\nã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ï¼š\n- es_dataï¼ˆå‰å‡¦ç†æ¸ˆã¿DataFrameï¼‰\n- TF-IDFè¡Œåˆ—ã¨Vectorizer\n- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°\n\nãŒCSVã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã€æ¬¡å›èµ·å‹•æ™‚ã«è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚\n\nâš ï¸ **æ³¨æ„**: ã‚»ãƒ«5ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ãŸå¾Œã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pickle\nimport numpy as np\nfrom scipy import sparse\n\n# appãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\nimport app as app_module\n\n# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\nif app_module.es_data is None or len(app_module.es_data) == 0:\n    print(\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n    print(\"ğŸ’¡ å…ˆã«ã‚»ãƒ«5ã‚’å®Ÿè¡Œã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¦ãã ã•ã„\")\nelse:\n    print(\"ğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...\")\n    print(f\"ğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n    \n    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n    os.makedirs(preprocessed_dir, exist_ok=True)\n    \n    # 1. es_dataã‚’ä¿å­˜\n    print(\"\\n  ğŸ’¾ es_data ã‚’ä¿å­˜ä¸­...\")\n    with open(preprocessed_files['es_data'], 'wb') as f:\n        pickle.dump(app_module.es_data, f)\n    size_mb = os.path.getsize(preprocessed_files['es_data']) / (1024 * 1024)\n    print(f\"  âœ… es_data ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n    \n    # 2. TF-IDFè¡Œåˆ—ã‚’ä¿å­˜\n    if app_module.tfidf_matrix is not None:\n        print(\"\\n  ğŸ’¾ tfidf_matrix ã‚’ä¿å­˜ä¸­...\")\n        sparse.save_npz(preprocessed_files['tfidf_matrix'], app_module.tfidf_matrix)\n        size_mb = os.path.getsize(preprocessed_files['tfidf_matrix']) / (1024 * 1024)\n        print(f\"  âœ… tfidf_matrix ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n    \n    # 3. Vectorizerã‚’ä¿å­˜\n    if app_module.vectorizer is not None:\n        print(\"\\n  ğŸ’¾ vectorizer ã‚’ä¿å­˜ä¸­...\")\n        with open(preprocessed_files['vectorizer'], 'wb') as f:\n            pickle.dump(app_module.vectorizer, f)\n        size_mb = os.path.getsize(preprocessed_files['vectorizer']) / (1024 * 1024)\n        print(f\"  âœ… vectorizer ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n    \n    # 4. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿å­˜\n    if 'semantic_embedding' in app_module.es_data.columns:\n        print(\"\\n  ğŸ’¾ semantic_embeddings ã‚’ä¿å­˜ä¸­...\")\n        embeddings_array = np.array(app_module.es_data['semantic_embedding'].tolist())\n        np.save(preprocessed_files['embeddings'], embeddings_array)\n        size_mb = os.path.getsize(preprocessed_files['embeddings']) / (1024 * 1024)\n        print(f\"  âœ… semantic_embeddings ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n    else:\n        print(\"\\n  âš ï¸ semantic_embeddingãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n    \n    # ä¿å­˜å®Œäº†ã‚µãƒãƒªãƒ¼\n    print(\"\\n\" + \"=\"*60)\n    print(\"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n    print(\"=\"*60)\n    print(f\"\\nğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n    print(\"\\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n    total_size = 0\n    for name, path in preprocessed_files.items():\n        if os.path.exists(path):\n            size_mb = os.path.getsize(path) / (1024 * 1024)\n            total_size += size_mb\n            print(f\"  âœ… {name}: {size_mb:.2f} MB\")\n    print(f\"\\nğŸ’¾ åˆè¨ˆã‚µã‚¤ã‚º: {total_size:.2f} MB\")\n    print(\"\\nğŸ’¡ æ¬¡å›èµ·å‹•æ™‚ã«ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™\")\n    print(\"âš¡ èµ·å‹•æ™‚é–“ãŒ 3-5åˆ† â†’ æ•°ç§’ ã«çŸ­ç¸®ã•ã‚Œã¾ã™ï¼\")\n    print(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n\n### å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œãªã„\n```\nå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ãŸã¯ãšãªã®ã«èªè­˜ã•ã‚Œãªã„\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- ã‚»ãƒ«4ã§å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n- ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèª\n- ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆes_data.pkl, tfidf_matrix.npz, vectorizer.pkl, embeddings.npyï¼‰ãŒæƒã£ã¦ã„ã‚‹ã‹ç¢ºèª\n- å†åº¦ã‚»ãƒ«6ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ç›´ã™\n\n### ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆãŒé…ã„ï¼ˆæ–°ã—ã„ãƒãƒƒãƒå‡¦ç†ç‰ˆï¼‰\n```\nã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã«æ™‚é–“ãŒã‹ã‹ã‚‹\n```\n\n**æ­£å¸¸ãªå‡¦ç†æ™‚é–“:**\n- **CPUç’°å¢ƒ**: 3-5åˆ†ï¼ˆ35,000ä»¶ã®å ´åˆï¼‰\n- **GPUç’°å¢ƒï¼ˆT4ï¼‰**: 30ç§’ã€œ1åˆ†\n- é€²æ—ãƒãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã€æ®‹ã‚Šæ™‚é–“ãŒç¢ºèªã§ãã¾ã™\n\n**å‡¦ç†æ™‚é–“ã®ç¢ºèªæ–¹æ³•:**\n- å®Ÿè¡Œæ™‚ã«ã€ŒğŸ§ª ãƒ†ã‚¹ãƒˆ: æœ€åˆã®100ä»¶ã‚’å‡¦ç†...ã€ã§äºˆæƒ³æ™‚é–“ãŒè¡¨ç¤ºã•ã‚Œã¾ã™\n- tqdmã®é€²æ—ãƒãƒ¼ã§æ®‹ã‚Šæ™‚é–“ã‚’ç¢ºèªã§ãã¾ã™\n\n**ã•ã‚‰ã«é«˜é€ŸåŒ–ã—ãŸã„å ´åˆ:**\n1. **å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨**: ã‚»ãƒ«6ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã€æ¬¡å›èµ·å‹•æ™‚ã«è‡ªå‹•èª­ã¿è¾¼ã¿ï¼ˆæ•°ç§’ã§èµ·å‹•ï¼‰\n2. Google Colabã§GPUï¼ˆT4ï¼‰ã‚’æœ‰åŠ¹åŒ–\n   - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ T4 GPU ã‚’é¸æŠ\n3. ãƒ‡ãƒ¼ã‚¿é‡ã‚’æ¸›ã‚‰ã™\n   ```python\n   # ã‚»ãƒ«4ã®å‰ã«å®Ÿè¡Œ\n   import pandas as pd\n   df = pd.read_csv(csv_path)\n   df = df.tail(10000)  # æœ€æ–°1ä¸‡ä»¶ã®ã¿\n   df.to_csv('temp.csv', index=False)\n   csv_path = 'temp.csv'\n   ```\n\n### sentence-transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒé…ã„\n```\nåˆå›ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- sentence-transformersã¯å¤§ããªãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆPyTorchã‚’å«ã‚€ï¼‰ã®ãŸã‚ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«5-10åˆ†ã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™\n- é€²è¡Œä¸­ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚Œã°æ­£å¸¸ã§ã™\n- å®Œäº†ã‚’å¾…ã£ã¦ãã ã•ã„\n\n### Sentence-BERTãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé…ã„\n```\nåˆå›å®Ÿè¡Œæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- åˆå›å®Ÿè¡Œæ™‚ã®ã¿ã€ãƒ¢ãƒ‡ãƒ«ï¼ˆç´„120MBï¼‰ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™\n- 2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚é«˜é€Ÿã§ã™\n- ã‚»ãƒ«5ã§ã€ŒğŸ“¥ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...ã€ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚Œã°æ­£å¸¸ã§ã™\n\n### é€²æ—ãƒãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„\n```\ntqdmã®é€²æ—ãƒãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- Google Colabã«ã¯tqdmãŒæ¨™æº–ã§å«ã¾ã‚Œã¦ã„ã¾ã™ãŒã€è¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯å‹•ä½œã«å•é¡Œã‚ã‚Šã¾ã›ã‚“\n- ã€Œâš ï¸ tqdmãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚é€²æ—è¡¨ç¤ºãªã—ã§å®Ÿè¡Œã—ã¾ã™ã€‚ã€ã¨è¡¨ç¤ºã•ã‚Œã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶šã•ã‚Œã¾ã™\n\n### ngrokãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼\n```\nTimeoutError: The read operation timed out\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n1. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦æœ€åˆã‹ã‚‰å®Ÿè¡Œ\n2. åˆ¥ã®æ™‚é–“å¸¯ã«è©¦ã™ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ··é›‘æ™‚ã‚’é¿ã‘ã‚‹ï¼‰\n3. ã‚»ãƒ«1ã§å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼š\n   ```python\n   !pip install pyngrok --default-timeout=100 -q\n   ```\n\n### CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n- ã‚»ãƒ«3ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n- Google Driveã®å ´åˆã€ãƒã‚¦ãƒ³ãƒˆãŒæ­£ã—ãè¡Œã‚ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\n- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ã§å®Ÿéš›ã®ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨\n\n### ngrokèªè¨¼ã‚¨ãƒ©ãƒ¼\n```\nInvalid credentials\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- ã‚»ãƒ«5ã® `NGROK_TOKEN` ã‚’æ­£ã—ãè¨­å®šã—ã¦ãã ã•ã„\n- https://dashboard.ngrok.com/ ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å†ç¢ºèª\n- ãƒˆãƒ¼ã‚¯ãƒ³ã®å‰å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚„æ”¹è¡ŒãŒãªã„ã‹ç¢ºèª\n\n### ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼\n```\nMemoryError\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\n- sentence-transformersã‚‚å¤§é‡ã®ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã—ã¾ã™\n- Google Colab Proã®ä½¿ç”¨ã‚’æ¤œè¨\n- ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼š\n   ```python\n   import pandas as pd\n   df = pd.read_csv(csv_path)\n   df = df.tail(10000)  # æœ€æ–°1ä¸‡ä»¶ã®ã¿\n   df.to_csv('temp.csv', index=False)\n   csv_path = 'temp.csv'\n   ```\n\n### ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼\n```\nModuleNotFoundError: No module named 'app'\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- ã‚»ãƒ«2ã§ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ãŒæˆåŠŸã—ã¦ã„ã‚‹ã‹ç¢ºèª\n- `%cd es` ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n- ã‚»ãƒ«5ã‚’å†å®Ÿè¡Œ\n\n### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãŒä½¿ãˆãªã„å ´åˆ\n```\nâš ï¸ sentence-transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚TF-IDFã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n```\n\n**å¯¾å‡¦æ³•ï¼š**\n- ã‚¢ãƒ—ãƒªã¯å‹•ä½œã—ã¾ã™ãŒã€æ„å‘³çš„é¡ä¼¼æ€§ã¯ä½¿ã‚ã‚Œã¾ã›ã‚“\n- ã‚»ãƒ«1ã‚’å†å®Ÿè¡Œã—ã¦sentence-transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ\n\n---\n\n## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®è©³ç´°\n\n### æœ€é©åŒ–å†…å®¹\n1. **å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨**: åˆå›èµ·å‹•å¾Œã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã€æ¬¡å›ã¯æ•°ç§’ã§èµ·å‹•\n2. **ãƒãƒƒãƒå‡¦ç†**: 1ä»¶ãšã¤å‡¦ç† â†’ 32ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\n3. **è»½é‡ãƒ¢ãƒ‡ãƒ«**: 768æ¬¡å…ƒ â†’ 384æ¬¡å…ƒï¼ˆç²¾åº¦ã¯ã»ã¼åŒã˜ã€é€Ÿåº¦2å€ï¼‰\n4. **GPUå¯¾å¿œ**: CUDAåˆ©ç”¨å¯èƒ½æ™‚ã¯è‡ªå‹•çš„ã«GPUä½¿ç”¨\n5. **é€²æ—è¡¨ç¤º**: tqdmã§æ®‹ã‚Šæ™‚é–“ã‚’å¯è¦–åŒ–\n\n### å‡¦ç†æ™‚é–“ã®æ¯”è¼ƒï¼ˆ35,000ä»¶ã®å ´åˆï¼‰\n- **æ—§å®Ÿè£…ï¼ˆapplyï¼‰**: ç´„40åˆ†\n- **æ–°å®Ÿè£…ï¼ˆCPUãƒ»åˆå›ï¼‰**: 3-5åˆ†ï¼ˆç´„8-13å€é«˜é€ŸåŒ–ï¼‰\n- **æ–°å®Ÿè£…ï¼ˆGPU T4ãƒ»åˆå›ï¼‰**: 30ç§’ã€œ1åˆ†ï¼ˆç´„40-80å€é«˜é€ŸåŒ–ï¼‰\n- **å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼ˆ2å›ç›®ä»¥é™ï¼‰**: æ•°ç§’ï¼ˆç´„400-800å€é«˜é€ŸåŒ–ï¼‰\n\n### å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºç›®å®‰\n- **es_data.pkl**: ç´„50-100 MBï¼ˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã«ã‚ˆã‚‹ï¼‰\n- **tfidf_matrix.npz**: ç´„20-50 MB\n- **vectorizer.pkl**: ç´„5-10 MB\n- **embeddings.npy**: ç´„50-150 MB\n- **åˆè¨ˆ**: ç´„125-310 MBï¼ˆ35,000ä»¶ã®å ´åˆï¼‰",
   "metadata": {
    "id": "troubleshoot"
   }
  }
 ]
}
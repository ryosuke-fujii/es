{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ ESè¨ºæ–­ãƒ„ãƒ¼ãƒ« - Google Colabèµ·å‹•ç”¨ï¼ˆSimCSE-JAç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€ESè¨ºæ–­ãƒ„ãƒ¼ãƒ«ï¼ˆSimCSE-JAãƒ¢ãƒ‡ãƒ«ç‰ˆï¼‰ã‚’Google Colabä¸Šã§èµ·å‹•ã—ã¾ã™ã€‚\n",
    "\n",
    "**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**: pkshatech/simcse-ja-bert-base-clcmlpï¼ˆæ—¥æœ¬èªç‰¹åŒ–ãƒ»768æ¬¡å…ƒï¼‰\n",
    "\n",
    "## ğŸ“‹ å®Ÿè¡Œæ‰‹é †\n",
    "\n",
    "1. **ã‚»ãƒ«1**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **ã‚»ãƒ«2**: GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "3. **ã‚»ãƒ«3**: CSVãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆGoogle Drive / ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰\n",
    "4. **ã‚»ãƒ«4**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãƒ»èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»é«˜é€ŸåŒ–ï¼‰\n",
    "5. **ã‚»ãƒ«5**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•\n",
    "6. **ã‚»ãƒ«6**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ»æ¬¡å›èµ·å‹•ã‚’é«˜é€ŸåŒ–ï¼‰\n",
    "\n",
    "## âš¡ é«˜é€Ÿèµ·å‹•ã«ã¤ã„ã¦\n",
    "\n",
    "**åˆå›èµ·å‹•**: ã‚»ãƒ«1ã€œ5ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆ3-5åˆ†ï¼‰\n",
    "**2å›ç›®ä»¥é™**: ã‚»ãƒ«6ã§ä¿å­˜ã—ãŸå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ•°ç§’ã§èµ·å‹•ï¼\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«1: å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install fastapi uvicorn pandas numpy scikit-learn tqdm -q\n",
    "\n",
    "# sentence-transformers ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨ï¼‰\n",
    "print(\"ğŸ”§ sentence-transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "!pip install sentence-transformers -q\n",
    "print(\"âœ… sentence-transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# pyngrokã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰\n",
    "!pip install pyngrok -q --default-timeout=100\n",
    "\n",
    "print(\"âœ… å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(\"\\nğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:\")\n",
    "print(\"  - FastAPI + uvicorn (Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³)\")\n",
    "print(\"  - pandas (ãƒ‡ãƒ¼ã‚¿å‡¦ç†)\")\n",
    "print(\"  - numpy (æ•°å€¤è¨ˆç®—)\")\n",
    "print(\"  - scikit-learn (TF-IDFã€æ©Ÿæ¢°å­¦ç¿’)\")\n",
    "print(\"  - sentence-transformers (ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç”¨BERT)\")\n",
    "print(\"  - tqdm (é€²æ—è¡¨ç¤º)\")\n",
    "print(\"  - pyngrok (å…¬é–‹URLä½œæˆ)\")\n",
    "print(\"\\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–:\")\n",
    "print(\"  - SimCSE-JAãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: 768æ¬¡å…ƒã®æ—¥æœ¬èªç‰¹åŒ–ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°\")\n",
    "print(\"  - ãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–\")\n",
    "print(\"  - GPUå¯¾å¿œï¼ˆT4æœ‰åŠ¹æ™‚ã¯ã•ã‚‰ã«é«˜é€Ÿï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«2: GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/ryosuke-fujii/es.git\n",
    "\n",
    "# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´\n",
    "%cd es\n",
    "\n",
    "print(\"âœ… ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(\"\\nğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ :\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«3: CSVãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "\n",
    "ä»¥ä¸‹ã®2ã¤ã®æ–¹æ³•ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š\n",
    "- **æ–¹æ³•A**: Google Driveã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆæ¨å¥¨ï¼‰\n",
    "- **æ–¹æ³•B**: ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ–¹æ³•A: Google Driveã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆæ¨å¥¨ï¼‰\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# çµ±åˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆå°±æ´»ä¼šè­° + ãƒ¯ãƒ³ã‚­ãƒ£ãƒªã‚¢ï¼‰\n",
    "csv_path = \"/content/drive/MyDrive/ä¼ç”»ãƒ»ãƒãƒ¼ã‚±ãƒãƒ¼ãƒ /gaxi_è‡ªå‹•åŒ–/Python/ESãƒ‡ãƒ¼ã‚¿æ•´å½¢/unified_es_data.csv\"\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {csv_path}\")\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ç¢ºèª\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶\")\n",
    "    if 'data_source' in df.columns:\n",
    "        print(\"\\nãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†…è¨³:\")\n",
    "        print(df['data_source'].value_counts())\n",
    "else:\n",
    "    print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}\")\n",
    "    print(\"\\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ©ã‚¦ã‚¶ã§ç›®çš„ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å³ã‚¯ãƒªãƒƒã‚¯ â†’ 'ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ–¹æ³•B: ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "# ============================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—\n",
    "csv_path = list(uploaded.keys())[0]\n",
    "print(f\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«4: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãƒ»èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "å‰å›å®Ÿè¡Œæ™‚ã«ä¿å­˜ã—ãŸå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ã€ãªã‘ã‚Œã°æ–°è¦ä½œæˆã—ã¾ã™ã€‚\n",
    "ã“ã‚Œã«ã‚ˆã‚Š2å›ç›®ä»¥é™ã®èµ·å‹•ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ï¼ˆ3-5åˆ† â†’ æ•°ç§’ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# ============================================\n",
    "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹è¨­å®š\n",
    "# ============================================\n",
    "# Google Driveã®æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨\n",
    "preprocessed_dir = \"/content/drive/MyDrive/ä¼ç”»ãƒ»ãƒãƒ¼ã‚±ãƒãƒ¼ãƒ /gaxi_è‡ªå‹•åŒ–/Python/ESãƒ‡ãƒ¼ã‚¿æ•´å½¢/es_preprocessed_data\"\n",
    "data_basename = \"unified_es_data_20251109\"\n",
    "\n",
    "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆ20251120ç‰ˆï¼‰\n",
    "preprocessed_files = {\n",
    "    'es_data': os.path.join(preprocessed_dir, 'unified_es_data_20251120_es_data.pkl'),\n",
    "    'tfidf_matrix': os.path.join(preprocessed_dir, 'unified_es_data_20251120_tfidf_matrix.npz'),\n",
    "    'vectorizer': os.path.join(preprocessed_dir, 'unified_es_data_20251120_vectorizer.pkl'),\n",
    "    'embeddings': os.path.join(preprocessed_dir, 'unified_es_data_20251120_embeddings.npy')\n",
    "}\n",
    "\n",
    "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "all_files_exist = all(os.path.exists(f) for f in preprocessed_files.values())\n",
    "\n",
    "if all_files_exist:\n",
    "    print(\"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ã—ã¾ã—ãŸï¼\")\n",
    "    print(f\"ğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n",
    "    print(\"\\nğŸ“ åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "    for name, path in preprocessed_files.items():\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        print(f\"  - {name}: {size_mb:.2f} MB\")\n",
    "    print(\"\\nğŸ’¡ ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã¨èµ·å‹•ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ï¼ˆæ•°ç§’ã§å®Œäº†ï¼‰\")\n",
    "    USE_PREPROCESSED = True\n",
    "else:\n",
    "    print(\"â„¹ï¸ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"â³ åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ3-5åˆ†ç¨‹åº¦ï¼‰\")\n",
    "    print(f\"ğŸ’¾ å®Œäº†å¾Œã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»¥ä¸‹ã«ä¿å­˜ã—ã¾ã™:\")\n",
    "    print(f\"   {preprocessed_dir}\")\n",
    "    USE_PREPROCESSED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«5: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ ï¼ˆé‡è¦ï¼ï¼‰\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆSimCSE-JAç‰ˆï¼‰\n",
    "from app_simcse_ja import app, load_csv_data, es_data, vectorizer, tfidf_matrix, sentence_model\n",
    "\n",
    "# ============================================\n",
    "# ngrokã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š\n",
    "# ============================================\n",
    "# https://dashboard.ngrok.com/ ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¦ãã ã•ã„\n",
    "\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN\"  # ã“ã“ã«ã‚ãªãŸã®ngrokãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›\n",
    "\n",
    "# ngrokã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰\n",
    "print(\"ğŸ”§ ngrokã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...\")\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        from pyngrok import ngrok\n",
    "        ngrok.set_auth_token(NGROK_TOKEN)\n",
    "        print(\"âœ… ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¾ã—ãŸ\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < 2:\n",
    "            print(f\"âš ï¸ ãƒªãƒˆãƒ©ã‚¤ {attempt + 1}/3...\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"âŒ ngrokã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "            print(\"\\nğŸ’¡ è§£æ±ºæ–¹æ³•:\")\n",
    "            print(\"1. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã‚‚ã†ä¸€åº¦è©¦ã™\")\n",
    "            print(\"2. ã‚»ãƒ«1ã‚’å†å®Ÿè¡Œã—ã¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "            raise\n",
    "\n",
    "# ============================================\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯æ–°è¦ï¼‰\n",
    "# ============================================\n",
    "\n",
    "# USE_PREPROCESSEDãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š\n",
    "if 'USE_PREPROCESSED' not in globals():\n",
    "    USE_PREPROCESSED = False\n",
    "    preprocessed_files = {}\n",
    "    preprocessed_dir = \"\"\n",
    "    print(\"â„¹ï¸ ã‚»ãƒ«4ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãŸã‚ã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“\")\n",
    "    print(\"â³ æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ3-5åˆ†ç¨‹åº¦ï¼‰\\n\")\n",
    "\n",
    "if USE_PREPROCESSED:\n",
    "    print(\"\\nâš¡ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    print(\"ğŸ’¡ ã“ã‚Œã«ã‚ˆã‚Šèµ·å‹•ãŒå¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¾ã™\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«appãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å–å¾—\n",
    "    import app_simcse_ja as app_module\n",
    "    \n",
    "    # es_dataã‚’èª­ã¿è¾¼ã¿\n",
    "    with open(preprocessed_files['es_data'], 'rb') as f:\n",
    "        app_module.es_data = pickle.load(f)\n",
    "    print(f\"  âœ… es_data: {len(app_module.es_data)}ä»¶\")\n",
    "    \n",
    "    # TF-IDFè¡Œåˆ—ã‚’èª­ã¿è¾¼ã¿\n",
    "    app_module.tfidf_matrix = sparse.load_npz(preprocessed_files['tfidf_matrix'])\n",
    "    print(f\"  âœ… tfidf_matrix: {app_module.tfidf_matrix.shape}\")\n",
    "    \n",
    "    # Vectorizerã‚’èª­ã¿è¾¼ã¿\n",
    "    with open(preprocessed_files['vectorizer'], 'rb') as f:\n",
    "        app_module.vectorizer = pickle.load(f)\n",
    "    print(f\"  âœ… vectorizer\")\n",
    "    \n",
    "    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿\n",
    "    if os.path.exists(preprocessed_files['embeddings']):\n",
    "        # SimCSE-JAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—ã«ã¯ä¸è¦ã ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯å¿…è¦ï¼‰\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            if app_module.sentence_model is None:\n",
    "                print(\"  ğŸ“¥ SimCSE-JAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "                app_module.sentence_model = SentenceTransformer('pkshatech/simcse-ja-bert-base-clcmlp')\n",
    "                print(\"  âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n",
    "        except ImportError:\n",
    "            print(\"  âš ï¸ sentence-transformersãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¯ç„¡åŠ¹ã§ã™ã€‚\")\n",
    "        \n",
    "        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿\n",
    "        app_module.es_data['semantic_embedding'] = list(np.load(preprocessed_files['embeddings']))\n",
    "        print(f\"  âœ… semantic_embeddings: {len(app_module.es_data['semantic_embedding'])}ä»¶\")\n",
    "    \n",
    "    print(\"\\nâœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆæ•°ç§’ã§å®Œäº†ï¼‰\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸ“‚ CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    print(\"â³ åˆå›å®Ÿè¡Œæ™‚ã¯SimCSE-JAãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ•°åˆ†ã‹ã‹ã‚Šã¾ã™...\")\n",
    "    print(\"âš¡ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã¯3-5åˆ†ç¨‹åº¦ã§å®Œäº†ã—ã¾ã™ï¼ˆãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–æ¸ˆã¿ï¼‰\")\n",
    "    print(\"\")\n",
    "    load_csv_data(csv_path)\n",
    "    print(\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ============================================\n",
    "# FastAPIã‚¢ãƒ—ãƒªã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
    "# ============================================\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "\n",
    "def run_fastapi():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "fastapi_thread = Thread(target=run_fastapi, daemon=True)\n",
    "fastapi_thread.start()\n",
    "\n",
    "print(\"\\nğŸš€ FastAPIã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã—ãŸ (ãƒãƒ¼ãƒˆ8000)\")\n",
    "\n",
    "# ============================================\n",
    "# ngrokãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆ\n",
    "# ============================================\n",
    "time.sleep(3)  # FastAPIã®èµ·å‹•ã‚’å¾…ã¤\n",
    "\n",
    "print(\"\\nğŸŒ ngrokãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆä¸­...\")\n",
    "try:\n",
    "    public_url = ngrok.connect(8000)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒèµ·å‹•ã—ã¾ã—ãŸï¼ï¼ˆSimCSE-JAç‰ˆï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸŒ å…¬é–‹URL: {public_url}\")\n",
    "    print(\"\\nğŸ’¡ ä¸Šè¨˜ã®URLã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„\")\n",
    "    print(\"\\nâš ï¸ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‰ã˜ã‚‹ã¨ã‚¢ãƒ—ãƒªã‚‚åœæ­¢ã—ã¾ã™\")\n",
    "    print(\"\\nğŸ” æ­è¼‰æ©Ÿèƒ½:\")\n",
    "    print(\"  - TF-IDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°\")\n",
    "    print(\"  - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆSimCSE-JAï¼‰ã§æ„å‘³çš„é¡ä¼¼æ€§ã‚’ç†è§£\")\n",
    "    print(\"  - STARæ§‹é€ åˆ†æã§ESå“è³ªã‚’è©•ä¾¡\")\n",
    "    \n",
    "    if USE_PREPROCESSED:\n",
    "        print(\"\\nâš¡ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨:\")\n",
    "        print(\"  - èµ·å‹•æ™‚é–“: æ•°ç§’ï¼ˆé€šå¸¸3-5åˆ†ã®ã¨ã“ã‚ï¼‰\")\n",
    "        print(f\"  - ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {preprocessed_dir}\")\n",
    "    else:\n",
    "        print(\"\\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:\")\n",
    "        print(\"  - ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”ŸæˆãŒ40åˆ†â†’3-5åˆ†ã«çŸ­ç¸®\")\n",
    "        print(\"  - SimCSE-JAãƒ¢ãƒ‡ãƒ«æ¡ç”¨: 768æ¬¡å…ƒã®æ—¥æœ¬èªç‰¹åŒ–ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°\")\n",
    "        print(\"  - GPUè‡ªå‹•æ¤œå‡º: T4æœ‰åŠ¹æ™‚ã¯30ç§’ã€œ1åˆ†ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ\")\n",
    "        print(f\"\\nğŸ’¾ æ¬¡å›èµ·å‹•ç”¨ã«å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã«ã¯ã€ã‚»ãƒ«6ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ngrokãƒˆãƒ³ãƒãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "    print(\"\\nğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n",
    "    print(\"1. NGROK_TOKENãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "    print(\"2. https://dashboard.ngrok.com/ ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å†ç¢ºèª\")\n",
    "    print(\"3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚»ãƒ«6: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "**æ¬¡å›èµ·å‹•ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã€å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’Google Driveã«ä¿å­˜ã—ã¾ã™ã€‚**\n",
    "\n",
    "ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã¨ï¼š\n",
    "- es_dataï¼ˆå‰å‡¦ç†æ¸ˆã¿DataFrameï¼‰\n",
    "- TF-IDFè¡Œåˆ—ã¨Vectorizer\n",
    "- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°\n",
    "\n",
    "ãŒæŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã€æ¬¡å›èµ·å‹•æ™‚ã«è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "âš ï¸ **æ³¨æ„**: ã‚»ãƒ«5ã§ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ãŸå¾Œã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# app_simcse_jaãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "import app_simcse_ja as app_module\n",
    "\n",
    "# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
    "if app_module.es_data is None or len(app_module.es_data) == 0:\n",
    "    print(\"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"ğŸ’¡ å…ˆã«ã‚»ãƒ«5ã‚’å®Ÿè¡Œã—ã¦ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(\"ğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...\")\n",
    "    print(f\"ğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n",
    "    \n",
    "    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. es_dataã‚’ä¿å­˜\n",
    "    print(\"\\n  ğŸ’¾ es_data ã‚’ä¿å­˜ä¸­...\")\n",
    "    with open(preprocessed_files['es_data'], 'wb') as f:\n",
    "        pickle.dump(app_module.es_data, f)\n",
    "    size_mb = os.path.getsize(preprocessed_files['es_data']) / (1024 * 1024)\n",
    "    print(f\"  âœ… es_data ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 2. TF-IDFè¡Œåˆ—ã‚’ä¿å­˜\n",
    "    if app_module.tfidf_matrix is not None:\n",
    "        print(\"\\n  ğŸ’¾ tfidf_matrix ã‚’ä¿å­˜ä¸­...\")\n",
    "        sparse.save_npz(preprocessed_files['tfidf_matrix'], app_module.tfidf_matrix)\n",
    "        size_mb = os.path.getsize(preprocessed_files['tfidf_matrix']) / (1024 * 1024)\n",
    "        print(f\"  âœ… tfidf_matrix ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 3. Vectorizerã‚’ä¿å­˜\n",
    "    if app_module.vectorizer is not None:\n",
    "        print(\"\\n  ğŸ’¾ vectorizer ã‚’ä¿å­˜ä¸­...\")\n",
    "        with open(preprocessed_files['vectorizer'], 'wb') as f:\n",
    "            pickle.dump(app_module.vectorizer, f)\n",
    "        size_mb = os.path.getsize(preprocessed_files['vectorizer']) / (1024 * 1024)\n",
    "        print(f\"  âœ… vectorizer ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # 4. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿å­˜\n",
    "    if 'semantic_embedding' in app_module.es_data.columns:\n",
    "        print(\"\\n  ğŸ’¾ semantic_embeddings ã‚’ä¿å­˜ä¸­...\")\n",
    "        embeddings_array = np.array(app_module.es_data['semantic_embedding'].tolist())\n",
    "        np.save(preprocessed_files['embeddings'], embeddings_array)\n",
    "        size_mb = os.path.getsize(preprocessed_files['embeddings']) / (1024 * 1024)\n",
    "        print(f\"  âœ… semantic_embeddings ä¿å­˜å®Œäº†: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"\\n  âš ï¸ semantic_embeddingãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰\")\n",
    "    \n",
    "    # ä¿å­˜å®Œäº†ã‚µãƒãƒªãƒ¼\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸ“‚ ä¿å­˜å…ˆ: {preprocessed_dir}\")\n",
    "    print(\"\\nğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "    total_size = 0\n",
    "    for name, path in preprocessed_files.items():\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            total_size += size_mb\n",
    "            print(f\"  âœ… {name}: {size_mb:.2f} MB\")\n",
    "    print(f\"\\nğŸ’¾ åˆè¨ˆã‚µã‚¤ã‚º: {total_size:.2f} MB\")\n",
    "    print(\"\\nğŸ’¡ æ¬¡å›èµ·å‹•æ™‚ã«ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè‡ªå‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™\")\n",
    "    print(\"âš¡ èµ·å‹•æ™‚é–“ãŒ 3-5åˆ† â†’ æ•°ç§’ ã«çŸ­ç¸®ã•ã‚Œã¾ã™ï¼\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

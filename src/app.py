# ============================================
# ã‚»ãƒ«3: ESè¨ºæ–­ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆFlaskçµ±åˆç‰ˆï¼‰
# ============================================
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from flask import Flask, request, jsonify, render_template, Response
import json
import re
import threading
import time
import os

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
es_data = None
vectorizer = None
tfidf_matrix = None
sentence_model = None  # Sentence-BERTãƒ¢ãƒ‡ãƒ«

# é¸æŠè‚¢ç”¨ãƒ‡ãƒ¼ã‚¿
universities_list = []
industries_list = []
companies_list = []
common_questions = []
company_counts = {}
industry_counts = {}

# æ¥­ç•Œã®å¤§åˆ†é¡ãƒªã‚¹ãƒˆ
INDUSTRY_MAJOR_CATEGORIES = [
    'ITãƒ»é€šä¿¡',
    'ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ç‰©æµãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼',
    'ã‚³ãƒ³ã‚µãƒ«ãƒ»ã‚·ãƒ³ã‚¯ã‚¿ãƒ³ã‚¯',
    'ã‚µãƒ¼ãƒ“ã‚¹',
    'ãƒ¡ãƒ¼ã‚«ãƒ¼ãƒ»è£½é€ æ¥­',
    'ä¸å‹•ç”£',
    'å•†ç¤¾ãƒ»å¸',
    'å°å£²ã‚Š',
    'åºƒå‘Šãƒ»ãƒã‚¹ã‚³ãƒŸ',
    'é‡‘è'
]

# ESãƒ†ãƒ¼ãƒã‚«ãƒ†ã‚´ãƒªä½“ç³»ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãï¼‰
ES_THEME_CATEGORIES = {
    # æ´»å‹•ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    'ç ”ç©¶ãƒ»å­¦è¡“æ´»å‹•': [
        'ç ”ç©¶', 'ã‚¼ãƒŸ', 'è«–æ–‡', 'å­¦ä¼š', 'å®Ÿé¨“', 'èª¿æŸ»', 'åˆ†æ', 'è€ƒå¯Ÿ'
    ],
    'ãƒ“ã‚¸ãƒã‚¹çµŒé¨“': [
        'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'é•·æœŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³', 'å®Ÿå‹™çµŒé¨“', 'è·å‹™çµŒé¨“', 'å–¶æ¥­', 'æ¥å®¢'
    ],
    'èª²å¤–æ´»å‹•': [
        'ã‚µãƒ¼ã‚¯ãƒ«', 'éƒ¨æ´»', 'å­¦ç”Ÿå›£ä½“', 'ãƒœãƒ©ãƒ³ãƒ†ã‚£ã‚¢', 'èª²å¤–', 'ã‚¹ãƒãƒ¼ãƒ„'
    ],
    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ã‚¹ã‚­ãƒ«
    'èª²é¡Œè§£æ±ºãƒ»æ”¹å–„': [
        'èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å…‹æœ', 'å¯¾ç­–', 'æ–½ç­–', 'æ‰“é–‹'
    ],
    'ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ãƒ»çµ„ç¹”é‹å–¶': [
        'ãƒªãƒ¼ãƒ€ãƒ¼', 'ä»£è¡¨', 'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'çµ±ç‡', 'çµ„ç¹”', 'é‹å–¶', 'ä¸»å°'
    ],
    'ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”åƒ': [
        'ãƒãƒ¼ãƒ ', 'ãƒ¡ãƒ³ãƒãƒ¼', 'å”åŠ›', 'é€£æº', 'å”åƒ', 'ã‚°ãƒ«ãƒ¼ãƒ—', 'å…±åŒ'
    ],
    'ä¼ç”»ãƒ»ææ¡ˆ': [
        'ä¼ç”»', 'ææ¡ˆ', 'ã‚¢ã‚¤ãƒ‡ã‚¢', 'ç«‹æ¡ˆ', 'æ–°è¦', 'ç™ºæ¡ˆ', 'ãƒ—ãƒ©ãƒ³'
    ],
    'æŠ€è¡“é–‹ç™ºãƒ»å‰µé€ ': [
        'é–‹ç™º', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚·ã‚¹ãƒ†ãƒ ', 'è¨­è¨ˆ', 'å®Ÿè£…', 'åˆ¶ä½œ', 'æ§‹ç¯‰'
    ],
    # ãƒã‚¤ãƒ³ãƒ‰ãƒ»å§¿å‹¢
    'æŒ‘æˆ¦ãƒ»ç›®æ¨™é”æˆ': [
        'æŒ‘æˆ¦', 'ç›®æ¨™', 'é”æˆ', 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸', 'æ–°ã—ã„', 'åˆã‚ã¦', 'æœªçµŒé¨“'
    ],
    'å›°é›£å…‹æœãƒ»é€†å¢ƒ': [
        'å›°é›£', 'å¤±æ•—', 'ä¹—ã‚Šè¶Šãˆ', 'è‹¦åŠ´', 'å£', 'é€†å¢ƒ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'å±æ©Ÿ'
    ],
    'æˆé•·ãƒ»å­¦ç¿’': [
        'æˆé•·', 'å­¦ã³', 'ç¿’å¾—', 'çµŒé¨“', 'æ°—ã¥ã', 'ç²å¾—', 'èº«ã«ã¤ã‘ãŸ'
    ],
    # æˆæœãƒ»ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ
    'å®šé‡çš„æˆæœ': [
        'å£²ä¸Š', 'å¢—åŠ ', 'å‰Šæ¸›', 'å‘ä¸Š', '%', 'å€', 'äºº', 'ä»¶', 'å††', 'é”æˆç‡'
    ],
    'ç¤¾ä¼šè²¢çŒ®ãƒ»å½±éŸ¿åŠ›': [
        'ç¤¾ä¼š', 'è²¢çŒ®', 'æ”¯æ´', 'åœ°åŸŸ', 'å½±éŸ¿', 'ä¾¡å€¤', 'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ'
    ],
    # å¿—æœ›å‹•æ©Ÿãƒ»ã‚­ãƒ£ãƒªã‚¢
    'ä¼æ¥­ç†è§£ãƒ»å…±æ„Ÿ': [
        'ç†å¿µ', 'ãƒ“ã‚¸ãƒ§ãƒ³', 'äº‹æ¥­', 'å¼·ã¿', 'é­…åŠ›', 'ç‰¹å¾´', 'å§¿å‹¢', 'å–ã‚Šçµ„ã¿'
    ],
    'ã‚­ãƒ£ãƒªã‚¢ãƒ“ã‚¸ãƒ§ãƒ³': [
        'å°†æ¥', 'ã‚­ãƒ£ãƒªã‚¢', 'å®Ÿç¾ã—ãŸã„', 'æˆã—é‚ã’ãŸã„', 'ç›®æŒ‡ã™', 'å¤¢'
    ]
}

# ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ†é¡ï¼ˆæ‹¡å¼µç‰ˆï¼‰
EPISODE_TYPES = {
    # ãƒ“ã‚¸ãƒã‚¹ãƒ»å®Ÿå‹™çµŒé¨“
    'ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»æ¥å®¢': {
        'keywords': [
            'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'ãƒã‚¤ãƒˆ', 'ãƒã‚¤ãƒˆå…ˆ', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆå…ˆ',
            'æ¥å®¢', 'è²©å£²', 'åº—èˆ—', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'é£²é£Ÿåº—',
            'ã‚³ãƒ³ãƒ“ãƒ‹', 'ã‚¹ãƒ¼ãƒ‘ãƒ¼', 'å°å£²', 'ãƒ›ãƒ¼ãƒ«', 'ãƒ¬ã‚¸'
        ],
        'weight': 1.0
    },
    'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ãƒ»å®Ÿå‹™': {
        'keywords': [
            'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã‚·ãƒƒãƒ—', 'é•·æœŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³',
            'å®Ÿå‹™', 'å®Ÿå‹™çµŒé¨“', 'è·å‹™çµŒé¨“', 'ãƒ“ã‚¸ãƒã‚¹çµŒé¨“',
            'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³å…ˆ', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ç”Ÿ'
        ],
        'weight': 1.0
    },
    'èµ·æ¥­ãƒ»äº‹æ¥­ç«‹ã¡ä¸Šã’': {
        'keywords': [
            'èµ·æ¥­', 'å‰µæ¥­', 'äº‹æ¥­', 'ãƒ“ã‚¸ãƒã‚¹',
            'ä¼šç¤¾è¨­ç«‹', 'æ³•äºº', 'ä»£è¡¨', 'çµŒå–¶',
            'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—', 'ãƒ™ãƒ³ãƒãƒ£ãƒ¼', 'è‡ªå–¶',
            'ã‚µãƒ¼ãƒ“ã‚¹ç«‹ã¡ä¸Šã’', 'äº‹æ¥­åŒ–', 'å•†å“é–‹ç™º'
        ],
        'weight': 1.0
    },

    # å­¦è¡“ãƒ»ç ”ç©¶æ´»å‹•
    'ç ”ç©¶ãƒ»ã‚¼ãƒŸæ´»å‹•': {
        'keywords': [
            'ç ”ç©¶', 'ã‚¼ãƒŸ', 'ã‚¼ãƒŸãƒŠãƒ¼ãƒ«', 'å®Ÿé¨“',
            'è«–æ–‡', 'å­¦ä¼š', 'å’è«–', 'ä¿®è«–',
            'ç ”ç©¶å®¤', 'ãƒ©ãƒœ', 'èª¿æŸ»', 'åˆ†æ',
            'è€ƒå¯Ÿ', 'ä»®èª¬', 'ãƒ‡ãƒ¼ã‚¿', 'æ¤œè¨¼'
        ],
        'weight': 1.0
    },
    'è³‡æ ¼å–å¾—ãƒ»å—é¨“': {
        'keywords': [
            'è³‡æ ¼', 'æ¤œå®š', 'è©¦é¨“', 'åˆæ ¼',
            'å‹‰å¼·', 'å—é¨“', 'å­¦ç¿’', 'TOEIC',
            'TOEFL', 'ç°¿è¨˜', 'å®…å»º', 'å…¬èªä¼šè¨ˆå£«',
            'FP', 'ã‚½ãƒ ãƒªã‚¨', 'å›½å®¶è©¦é¨“'
        ],
        'weight': 0.8
    },

    # èª²å¤–æ´»å‹•
    'éƒ¨æ´»å‹•ãƒ»ä½“è‚²ä¼š': {
        'keywords': [
            'éƒ¨æ´»', 'éƒ¨æ´»å‹•', 'ä½“è‚²ä¼š', 'é‹å‹•éƒ¨',
            'ç·´ç¿’', 'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', 'å¤§ä¼š', 'è©¦åˆ',
            'é¸æ‰‹', 'ã‚­ãƒ£ãƒ—ãƒ†ãƒ³', 'ä¸»å°†', 'ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼',
            'å…¨å›½å¤§ä¼š', 'åœ°åŒºå¤§ä¼š', 'çœŒå¤§ä¼š'
        ],
        'weight': 1.0
    },
    'ã‚µãƒ¼ã‚¯ãƒ«æ´»å‹•': {
        'keywords': [
            'ã‚µãƒ¼ã‚¯ãƒ«', 'ã‚µãƒ¼ã‚¯ãƒ«æ´»å‹•', 'åŒå¥½ä¼š',
            'æ–‡åŒ–ç³»', 'è¶£å‘³', 'æ„›å¥½ä¼š',
            'ã‚µãƒ¼ã‚¯ãƒ«ä»£è¡¨', 'ã‚µãƒ¼ã‚¯ãƒ«é•·'
        ],
        'weight': 1.0
    },
    'å­¦ç”Ÿå›£ä½“ãƒ»NPO': {
        'keywords': [
            'å­¦ç”Ÿå›£ä½“', 'å›£ä½“', 'NPO', 'NGO',
            'ãƒœãƒ©ãƒ³ãƒ†ã‚£ã‚¢', 'ç¤¾ä¼šè²¢çŒ®', 'æ”¯æ´æ´»å‹•',
            'åœ°åŸŸæ´»å‹•', 'ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£', 'å¸‚æ°‘æ´»å‹•',
            'å­¦ç”Ÿçµ„ç¹”', 'ä»£è¡¨', 'é‹å–¶'
        ],
        'weight': 1.0
    },

    # å›½éš›ãƒ»èªå­¦çµŒé¨“
    'ç•™å­¦ãƒ»æµ·å¤–çµŒé¨“': {
        'keywords': [
            'ç•™å­¦', 'æµ·å¤–', 'æµ·å¤–çµŒé¨“', 'æµ·å¤–ç•™å­¦',
            'äº¤æ›ç•™å­¦', 'èªå­¦ç•™å­¦', 'çŸ­æœŸç•™å­¦', 'é•·æœŸç•™å­¦',
            'æµ·å¤–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³', 'ãƒ›ãƒ¼ãƒ ã‚¹ãƒ†ã‚¤', 'æµ·å¤–ãƒœãƒ©ãƒ³ãƒ†ã‚£ã‚¢',
            'ç¾åœ°', 'ç•°æ–‡åŒ–', 'å¤–å›½', 'æ¸¡èˆª'
        ],
        'weight': 1.0
    },

    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ
    'ã‚³ãƒ³ãƒ†ã‚¹ãƒˆãƒ»å¤§ä¼š': {
        'keywords': [
            'ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ', 'ã‚³ãƒ³ãƒš', 'ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³',
            'å¤§ä¼š', 'ç«¶æŠ€ä¼š', 'ãƒãƒƒã‚«ã‚½ãƒ³',
            'ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ', 'ãƒ—ãƒ¬ã‚¼ãƒ³å¤§ä¼š',
            'å…¥è³', 'å„ªå‹', 'å—è³', 'è¡¨å½°'
        ],
        'weight': 1.0
    },

    # å€‹äººãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
    'å€‹äººãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ»è¶£å‘³': {
        'keywords': [
            'å€‹äºº', 'è¶£å‘³', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ', 'åˆ¶ä½œ',
            'ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª', 'ã‚¢ãƒ—ãƒªé–‹ç™º', 'Webåˆ¶ä½œ',
            'ãƒ–ãƒ­ã‚°', 'SNS', 'YouTube', 'å‹•ç”»',
            'ä½œå“', 'ãƒãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ‰', 'DIY'
        ],
        'weight': 0.8
    },

    # æ•™è‚²é–¢é€£
    'å®¶åº­æ•™å¸«ãƒ»å¡¾è¬›å¸«': {
        'keywords': [
            'å®¶åº­æ•™å¸«', 'å¡¾', 'å¡¾è¬›å¸«', 'è¬›å¸«',
            'æŒ‡å°', 'æ•™è‚²', 'ç”Ÿå¾’', 'æ•™ãˆã‚‹',
            'æˆæ¥­', 'æ·»å‰Š', 'é€²è·¯æŒ‡å°'
        ],
        'weight': 1.0
    },

    # ãã®ä»–
    'ãã®ä»–ã®çµŒé¨“': {
        'keywords': [],
        'weight': 0.5
    }
}

# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
# templatesãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã‚€
base_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(os.path.dirname(base_dir), 'templates')
app = Flask(__name__, template_folder=template_dir)

# ============================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
# ============================================

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if pd.isna(text):
        return ""
    text = re.sub(r'\n+', ' ', str(text))
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'ç¶šãã‚’èª­ã‚€.*', '', text)
    text = re.sub(r'å•é¡Œã‚’å ±å‘Šã™ã‚‹.*', '', text)
    return text.strip()

def remove_prefix(text):
    """ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤"""
    if pd.isna(text):
        return ""
    return re.sub(r'^[^ï¼š]+ï¼š\s*', '', str(text))

def extract_university(user_info):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‹ã‚‰å¤§å­¦åã‚’æŠ½å‡º"""
    if pd.isna(user_info):
        return "ä¸æ˜"
    match = re.search(r'\d{2}å’\s*\|\s*([^|]+)\s*\|', str(user_info))
    if match:
        return match.group(1).strip()
    return "ä¸æ˜"

def extract_major_industry_category(industry):
    """æ¥­ç•Œåã‹ã‚‰å¤§åˆ†é¡ã‚’æŠ½å‡º"""
    if pd.isna(industry) or not industry:
        return None

    industry_str = str(industry).strip()

    # å¤§åˆ†é¡ãƒªã‚¹ãƒˆã‹ã‚‰ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’æ¢ã™
    for major_category in INDUSTRY_MAJOR_CATEGORIES:
        if industry_str.startswith(major_category):
            return major_category

    return None

def categorize_es_themes(text):
    """ESã®ãƒ†ãƒ¼ãƒã‚’ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã§åˆ¤å®š"""
    if pd.isna(text) or not text:
        return []

    text_str = str(text)
    matched_themes = []

    for theme_name, keywords in ES_THEME_CATEGORIES.items():
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        keyword_count = sum(1 for kw in keywords if kw in text_str)

        # é–¾å€¤ã‚’è¶…ãˆãŸã‚‰ãƒ†ãƒ¼ãƒã¨ã—ã¦èªå®šï¼ˆ2å€‹ä»¥ä¸Šï¼‰
        if keyword_count >= 2:
            matched_themes.append({
                'theme': theme_name,
                'score': keyword_count
            })

    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    matched_themes.sort(key=lambda x: x['score'], reverse=True)

    return matched_themes if matched_themes else [{'theme': 'ãã®ä»–', 'score': 0}]

def extract_theme_keywords_for_weighting(text):
    """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é‡ã¿ä»˜ã‘ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    if pd.isna(text) or not text:
        return str(text)

    text_str = str(text)
    weighted_text = text_str

    # ãƒ†ãƒ¼ãƒåˆ¥ã«é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦é‡ã¿ä»˜ã‘
    for theme_name, keywords in ES_THEME_CATEGORIES.items():
        for keyword in keywords:
            if keyword in text_str:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3å›ç¹°ã‚Šè¿”ã—ã¦é‡è¦åº¦ã‚’ä¸Šã’ã‚‹
                weighted_text += f" {keyword} {keyword} {keyword}"

    return weighted_text

def analyze_es_structure(text):
    """ESã®æ§‹é€ ã‚’åˆ†æã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆSTARãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰"""
    if pd.isna(text) or not text:
        return {
            'situation': 0,
            'task': 0,
            'action': 0,
            'result': 0,
            'learning': 0
        }

    text_str = str(text)

    structure_features = {
        'situation': 0,  # çŠ¶æ³èª¬æ˜
        'task': 0,       # èª²é¡Œãƒ»ç›®æ¨™
        'action': 0,     # å…·ä½“çš„è¡Œå‹•
        'result': 0,     # æˆæœãƒ»çµæœ
        'learning': 0    # å­¦ã³
    }

    # çŠ¶æ³èª¬æ˜ã®æ¤œå‡º
    situation_keywords = ['ã«ãŠã„ã¦', 'ã§', 'ã«æ‰€å±', 'ã«å‚åŠ ', 'å½“æ™‚', 'ã§ã¯', 'ã¨ã—ã¦']
    structure_features['situation'] = sum(1 for kw in situation_keywords if kw in text_str)

    # èª²é¡Œãƒ»ç›®æ¨™ã®æ¤œå‡º
    task_keywords = ['ç›®æ¨™', 'èª²é¡Œ', 'ã—ãŸã„', 'ã‚’ç›®æŒ‡', 'æ”¹å–„', 'å‘ä¸Š', 'å•é¡Œ', 'å¿…è¦']
    structure_features['task'] = sum(1 for kw in task_keywords if kw in text_str)

    # å…·ä½“çš„è¡Œå‹•ã®æ¤œå‡º
    action_keywords = ['ç§ã¯', 'å–ã‚Šçµ„ã‚“ã ', 'å®Ÿæ–½', 'å·¥å¤«', 'ææ¡ˆ', 'å°å…¥', 'è¡Œã£ãŸ', 'è€ƒãˆãŸ']
    structure_features['action'] = sum(1 for kw in action_keywords if kw in text_str)

    # æˆæœã®æ¤œå‡º
    result_keywords = ['çµæœ', 'é”æˆ', 'å‘ä¸Š', '%', 'å¢—åŠ ', 'æˆåŠŸ', 'å®Ÿç¾', 'å®Œæˆ']
    structure_features['result'] = sum(1 for kw in result_keywords if kw in text_str)

    # å­¦ã³ã®æ¤œå‡º
    learning_keywords = ['å­¦ã‚“ã ', 'å¾—ãŸ', 'èº«ã«ã¤ã‘ãŸ', 'æ°—ã¥ã„ãŸ', 'çµŒé¨“ã‹ã‚‰', 'ç†è§£ã—ãŸ', 'æˆé•·']
    structure_features['learning'] = sum(1 for kw in learning_keywords if kw in text_str)

    return structure_features

def classify_episode_type(text):
    """
    ESãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š

    Args:
        text (str): ESæœ¬æ–‡

    Returns:
        dict: {
            'type': ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—å,
            'confidence': ä¿¡é ¼åº¦ï¼ˆãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°ï¼‰,
            'matched_keywords': ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        }
    """
    if pd.isna(text) or not text:
        return {
            'type': 'ãã®ä»–ã®çµŒé¨“',
            'confidence': 0,
            'matched_keywords': []
        }

    text_str = str(text)

    # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã§ãƒãƒƒãƒãƒ³ã‚°
    episode_scores = []

    for episode_type, config in EPISODE_TYPES.items():
        keywords = config['keywords']
        weight = config['weight']

        # ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        matched_keywords = [kw for kw in keywords if kw in text_str]
        match_count = len(matched_keywords)

        if match_count > 0:
            # ã‚¹ã‚³ã‚¢ = ãƒãƒƒãƒæ•° Ã— é‡ã¿
            score = match_count * weight
            episode_scores.append({
                'type': episode_type,
                'score': score,
                'confidence': match_count,
                'matched_keywords': matched_keywords[:5]  # æœ€å¤§5å€‹ã¾ã§
            })

    # ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ã„ã‚‚ã®ã‚’è¿”ã™
    if episode_scores:
        best_match = max(episode_scores, key=lambda x: x['score'])
        return {
            'type': best_match['type'],
            'confidence': best_match['confidence'],
            'matched_keywords': best_match['matched_keywords']
        }

    # ãƒãƒƒãƒã—ãªã„å ´åˆ
    return {
        'type': 'ãã®ä»–ã®çµŒé¨“',
        'confidence': 0,
        'matched_keywords': []
    }

def classify_multiple_episode_types(text, top_n=2):
    """
    è¤‡æ•°ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™ï¼ˆãƒãƒ«ãƒãƒ©ãƒ™ãƒ«å¯¾å¿œï¼‰

    Args:
        text (str): ESæœ¬æ–‡
        top_n (int): è¿”ã™ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®æœ€å¤§æ•°

    Returns:
        list: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
    """
    if pd.isna(text) or not text:
        return [{'type': 'ãã®ä»–ã®çµŒé¨“', 'confidence': 0}]

    text_str = str(text)

    episode_scores = []

    for episode_type, config in EPISODE_TYPES.items():
        keywords = config['keywords']
        weight = config['weight']

        matched_keywords = [kw for kw in keywords if kw in text_str]
        match_count = len(matched_keywords)

        if match_count > 0:
            score = match_count * weight
            episode_scores.append({
                'type': episode_type,
                'score': score,
                'confidence': match_count
            })

    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    episode_scores.sort(key=lambda x: x['score'], reverse=True)

    # ä¸Šä½top_nã‚’è¿”ã™
    if episode_scores:
        return episode_scores[:top_n]

    return [{'type': 'ãã®ä»–ã®çµŒé¨“', 'confidence': 0}]

def load_csv_data(csv_path):
    """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ•´å½¢"""
    global es_data, vectorizer, tfidf_matrix, sentence_model

    print(f"\nğŸ“‚ CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­...")

    es_data = pd.DataFrame({
        'company_name': df['p-company-summary__name'].apply(clean_text),
        'industry': df['p-company-summary__stage-sub'].apply(remove_prefix),
        'title': df['p-company-heading-contents__title'].apply(clean_text),
        'user_info': df.get('c-panel-variant2__header-user', pd.Series()).apply(clean_text),
        'question_1': df['u-font-light'].apply(clean_text),
        'answer_1': df['c-show-more__content'].apply(clean_text),
        'question_2': df.get('u-font-light (2)', pd.Series()).apply(clean_text),
        'answer_2': df.get('c-show-more__content (2)', pd.Series()).apply(clean_text),
        'question_3': df.get('u-font-light (3)', pd.Series()).apply(clean_text),
        'answer_3': df.get('c-show-more__content (3)', pd.Series()).apply(clean_text),
        'avg_salary': df.get('p-company-table (11)', pd.Series()).apply(clean_text),
        'employee_count': df.get('p-company-summary__stage-sub (3)', pd.Series()).apply(remove_prefix),
    })

    es_data['university'] = es_data['user_info'].apply(extract_university)

    es_data['result_status'] = es_data['title'].apply(
        lambda x: 'å†…å®š' if 'å†…å®š' in str(x) else ('é€šé' if 'é€šé' in str(x) else 'ä¸æ˜')
    )

    es_data = es_data[es_data['result_status'].isin(['é€šé', 'å†…å®š'])]

    es_data['combined_answer'] = (
        es_data['answer_1'].fillna('') + ' ' +
        es_data['answer_2'].fillna('') + ' ' +
        es_data['answer_3'].fillna('')
    ).str.strip()

    es_data = es_data[es_data['combined_answer'].str.len() > 50]

    print(f"âœ… æœ‰åŠ¹ãªESãƒ‡ãƒ¼ã‚¿: {len(es_data)}ä»¶")

    # ãƒ†ãƒ¼ãƒã‚«ãƒ†ã‚´ãƒªåˆ†æã‚’è¿½åŠ 
    print("ğŸ”§ ESã®ãƒ†ãƒ¼ãƒåˆ†æä¸­...")
    es_data['themes'] = es_data['combined_answer'].apply(categorize_es_themes)

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ†æã‚’è¿½åŠ 
    print("ğŸ”§ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ†æä¸­...")
    es_data['episode_type'] = es_data['combined_answer'].apply(classify_episode_type)
    es_data['episode_types_multi'] = es_data['combined_answer'].apply(
        lambda x: classify_multiple_episode_types(x, top_n=2)
    )

    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®çµ±è¨ˆã‚’å‡ºåŠ›
    episode_type_counts = {}
    for episode_info in es_data['episode_type']:
        episode_type = episode_info['type']
        episode_type_counts[episode_type] = episode_type_counts.get(episode_type, 0) + 1

    print("\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†å¸ƒ:")
    for episode_type, count in sorted(episode_type_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  - {episode_type}: {count}ä»¶")

    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡ã¿ä»˜ã‘ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    print("\nğŸ”§ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡ã¿ä»˜ã‘ä¸­...")
    es_data['weighted_answer'] = es_data['combined_answer'].apply(extract_theme_keywords_for_weighting)

    print("ğŸ”§ TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­ï¼ˆæœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰...")
    vectorizer = TfidfVectorizer(
        max_features=3000,  # 1000 â†’ 3000ã«å¢—åŠ 
        min_df=2,
        max_df=0.8,
        ngram_range=(1, 3)  # (1,2) â†’ (1,3)ã«æ‹¡å¼µ
    )

    # é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    tfidf_matrix = vectorizer.fit_transform(es_data['weighted_answer'])
    print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {tfidf_matrix.shape}")

    # ============================================
    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆSentence-BERTï¼‰
    # ============================================
    print("ğŸ”§ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­...")
    try:
        from sentence_transformers import SentenceTransformer
        import time

        # tqdmã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé€²æ—è¡¨ç¤ºç”¨ï¼‰
        try:
            from tqdm import tqdm
            has_tqdm = True
        except ImportError:
            print("  âš ï¸ tqdmãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚é€²æ—è¡¨ç¤ºãªã—ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
            has_tqdm = False

        # 1. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        if sentence_model is None:
            print("  ğŸ“¥ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆ384æ¬¡å…ƒã€ç´„2å€é€Ÿã„ï¼‰
            sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

            # GPUå¯¾å¿œï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            try:
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                print(f"  ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
                sentence_model = sentence_model.to(device)
            except ImportError:
                print("  ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cpu")

            print("  âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        # 2. ã¾ãš100ä»¶ã§ãƒ†ã‚¹ãƒˆï¼ˆæ‰€è¦æ™‚é–“ã®äºˆæ¸¬ï¼‰
        print("\n  ğŸ§ª ãƒ†ã‚¹ãƒˆ: æœ€åˆã®100ä»¶ã‚’å‡¦ç†ã—ã¦æ‰€è¦æ™‚é–“ã‚’äºˆæ¸¬...")
        test_start = time.time()
        test_count = min(100, len(es_data))
        test_texts = es_data['weighted_answer'].head(test_count).apply(lambda x: str(x)[:512]).tolist()
        test_embeddings = sentence_model.encode(
            test_texts,
            convert_to_tensor=False,
            show_progress_bar=False,
            batch_size=32
        )
        test_time = time.time() - test_start

        estimated_total_time = (test_time / test_count) * len(es_data)
        print(f"  â±ï¸  {test_count}ä»¶ã®å‡¦ç†æ™‚é–“: {test_time:.2f}ç§’")
        print(f"  ğŸ“Š äºˆæƒ³æ‰€è¦æ™‚é–“: {estimated_total_time / 60:.1f}åˆ†")

        # 3. å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå‡¦ç†
        print(f"\n  ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆä¸­ï¼ˆ{len(es_data)}ä»¶ï¼‰...")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§æº–å‚™ï¼ˆé•·ã•åˆ¶é™ã‚’512æ–‡å­—ã«ï¼‰
        all_texts = es_data['weighted_answer'].apply(lambda x: str(x)[:512]).tolist()

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨­å®š
        batch_size = 32  # CPUã®å ´åˆã¯16-32ãŒæœ€é©
        all_embeddings = []

        # ãƒãƒƒãƒå‡¦ç†ãƒ«ãƒ¼ãƒ—
        batch_range = range(0, len(all_texts), batch_size)
        if has_tqdm:
            batch_range = tqdm(batch_range, desc="  ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ")

        for i in batch_range:
            batch_texts = all_texts[i:i+batch_size]

            # ãƒãƒƒãƒã§ä¸€æ°—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            batch_embeddings = sentence_model.encode(
                batch_texts,
                convert_to_tensor=False,
                show_progress_bar=False,
                batch_size=batch_size
            )

            # ãƒªã‚¹ãƒˆã«è¿½åŠ 
            if isinstance(batch_embeddings, list):
                all_embeddings.extend(batch_embeddings)
            else:
                # numpyã‚¢ãƒ¬ã‚¤ã®å ´åˆ
                all_embeddings.extend(batch_embeddings.tolist() if hasattr(batch_embeddings, 'tolist') else list(batch_embeddings))

        # 4. DataFrameã«æ ¼ç´
        es_data['semantic_embedding'] = all_embeddings

        # 5. çµæœç¢ºèª
        print(f"\n  âœ… ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†ï¼ˆ{len(all_embeddings)}ä»¶ï¼‰")
        if len(all_embeddings) > 0:
            first_embedding = all_embeddings[0]
            if hasattr(first_embedding, '__len__'):
                print(f"  ğŸ“ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¬¡å…ƒ: {len(first_embedding)}")

    except ImportError:
        print("âš ï¸ sentence-transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("   pip install sentence-transformers ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        print("   TF-IDFã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        es_data['semantic_embedding'] = None

    except Exception as e:
        print(f"âš ï¸ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        es_data['semantic_embedding'] = None

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {es_data['company_name'].nunique()}")
    print(f"  - æ¥­ç•Œæ•°: {es_data['industry'].nunique()}")
    print(f"  - é€šéES: {(es_data['result_status'] == 'é€šé').sum()}ä»¶")
    print(f"  - å†…å®šES: {(es_data['result_status'] == 'å†…å®š').sum()}ä»¶")

    # é¸æŠè‚¢ã‚’æŠ½å‡º
    global universities_list, industries_list, companies_list, common_questions
    global company_counts, industry_counts

    print("\nğŸ“‹ é¸æŠè‚¢ã‚’æŠ½å‡ºä¸­...")

    universities_list = sorted(es_data['university'].dropna().unique().tolist())
    universities_list = [u for u in universities_list if u != "ä¸æ˜" and str(u).strip() != ""]

    industries_list = sorted(es_data['industry'].dropna().unique().tolist())
    industries_list = [i for i in industries_list if i and str(i).strip() != ""]

    # ä¼æ¥­ãƒªã‚¹ãƒˆã®å‡¦ç†ã‚’æ”¹å–„ï¼ˆå‰æ ªä¼æ¥­ã‚‚å«ã‚ã‚‹ï¼‰
    companies_list = es_data['company_name'].dropna().unique().tolist()
    # ç©ºæ–‡å­—ã‚„ç©ºç™½ã®ã¿ã®ä¼æ¥­åã‚’é™¤å¤–ï¼ˆãŸã ã—ã€å‰æ ªä¼æ¥­ã¯ä¿æŒï¼‰
    companies_list = [c for c in companies_list if c and str(c).strip() != "" and len(str(c).strip()) > 1]
    # ã‚½ãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
    companies_list = sorted(companies_list, key=lambda x: str(x))

    # ä¼æ¥­ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    company_counts = es_data['company_name'].value_counts().to_dict()

    # æ¥­ç•Œã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    industry_counts = es_data['industry'].value_counts().to_dict()

    common_questions = [
        "å­¦ç”Ÿæ™‚ä»£ã«åŠ›ã‚’å…¥ã‚ŒãŸã“ã¨ï¼ˆã‚¬ã‚¯ãƒã‚«ï¼‰",
        "å¿—æœ›å‹•æ©Ÿ",
        "è‡ªå·±PR",
        "ã‚ãªãŸã®å¼·ã¿ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰",
        "æŒ‘æˆ¦ã—ãŸã“ã¨ãƒ»ãƒãƒ£ãƒ¬ãƒ³ã‚¸",
        "å›°é›£ã‚’ä¹—ã‚Šè¶ŠãˆãŸçµŒé¨“",
        "ãƒãƒ¼ãƒ ã§æˆæœã‚’å‡ºã—ãŸçµŒé¨“",
        "ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚’ç™ºæ®ã—ãŸçµŒé¨“",
        "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§å­¦ã³ãŸã„ã“ã¨",
        "å°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ“ã‚¸ãƒ§ãƒ³"
    ]

    print(f"âœ… é¸æŠè‚¢ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"  - å¤§å­¦: {len(universities_list)}æ ¡")
    print(f"  - æ¥­ç•Œ: {len(industries_list)}ç¨®é¡")
    print(f"  - ä¼æ¥­: {len(companies_list)}ç¤¾")

def calculate_similarity(input_text, top_n=100):
    """é¡ä¼¼åº¦è¨ˆç®—ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼šTF-IDF + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ + æ§‹é€ åˆ†æï¼‰"""
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«ã‚‚åŒã˜é‡ã¿ä»˜ã‘ã‚’é©ç”¨
    weighted_input = extract_theme_keywords_for_weighting(input_text)

    # TF-IDFé¡ä¼¼åº¦
    input_vector = vectorizer.transform([weighted_input])
    tfidf_similarities = cosine_similarity(input_vector, tfidf_matrix)[0]

    # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦ï¼ˆBERTï¼‰
    semantic_similarities = np.zeros(len(es_data))
    has_semantic = False

    try:
        from sentence_transformers import SentenceTransformer

        if sentence_model is not None and es_data['semantic_embedding'].iloc[0] is not None:
            # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
            input_embedding = sentence_model.encode(str(input_text)[:512], convert_to_tensor=False)

            # å…¨ESã¨ã®é¡ä¼¼åº¦è¨ˆç®—
            embeddings_matrix = np.vstack(es_data['semantic_embedding'].values)
            semantic_similarities = cosine_similarity([input_embedding], embeddings_matrix)[0]
            has_semantic = True
    except Exception as e:
        print(f"âš ï¸ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãŒä½¿ãˆã‚‹å ´åˆã¯é‡è¦–ï¼‰
    if has_semantic:
        combined_similarities = (
            tfidf_similarities * 0.3 +      # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
            semantic_similarities * 0.7     # æ„å‘³ãƒãƒƒãƒ
        )
    else:
        combined_similarities = tfidf_similarities

    # æ§‹é€ åˆ†æã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    input_structure = analyze_es_structure(input_text)

    result = es_data.copy()
    result['similarity_score'] = combined_similarities

    # ä¸Šä½å€™è£œã«å¯¾ã—ã¦æ§‹é€ é¡ä¼¼åº¦ã‚’è¨ˆç®—
    result = result.sort_values('similarity_score', ascending=False).head(top_n * 2)

    # æ§‹é€ é¡ä¼¼åº¦ã‚’è¿½åŠ 
    structure_scores = []
    for idx, row in result.iterrows():
        es_structure = analyze_es_structure(row['combined_answer'])

        # æ§‹é€ ã®ä¸€è‡´åº¦ã‚’è¨ˆç®—
        structure_similarity = sum(
            min(input_structure[key], es_structure[key])
            for key in input_structure.keys()
        ) / max(sum(input_structure.values()), 1)

        structure_scores.append(structure_similarity)

    result['structure_score'] = structure_scores

    # æœ€çµ‚ã‚¹ã‚³ã‚¢ = å†…å®¹é¡ä¼¼åº¦ * 0.8 + æ§‹é€ é¡ä¼¼åº¦ * 0.2
    result['similarity_score'] = (
        result['similarity_score'] * 0.8 +
        result['structure_score'] * 0.2
    )

    # ãƒ†ãƒ¼ãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–
    # å…¥åŠ›ESã®ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
    input_themes = categorize_es_themes(input_text)
    input_theme_names = set([t['theme'] for t in input_themes[:3]])  # ä¸Šä½3ãƒ†ãƒ¼ãƒ

    # ãƒ†ãƒ¼ãƒãƒœãƒ¼ãƒŠã‚¹ã‚’è¿½åŠ 
    for idx, row in result.iterrows():
        es_themes = row['themes']
        es_theme_names = set([t['theme'] for t in es_themes[:3]])

        # ãƒ†ãƒ¼ãƒã®ä¸€è‡´æ•°ã‚’è¨ˆç®—
        theme_overlap = len(input_theme_names & es_theme_names)

        # ãƒœãƒ¼ãƒŠã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0ã€œ0.15ï¼‰
        # 3ã¤ä¸€è‡´ â†’ +15%ã€2ã¤ä¸€è‡´ â†’ +10%ã€1ã¤ä¸€è‡´ â†’ +5%
        theme_bonus = theme_overlap * 0.05

        # ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°ï¼ˆä¹—ç®—ï¼‰
        result.at[idx, 'similarity_score'] = (
            row['similarity_score'] * (1 + theme_bonus)
        )

    # æœ€çµ‚çš„ã«top_nã«çµã‚‹
    result = result.sort_values('similarity_score', ascending=False).head(top_n)

    return result

def extract_salary_numeric(salary_str):
    """çµ¦ä¸ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if pd.isna(salary_str):
        return None
    match = re.search(r'(\d+)ä¸‡', str(salary_str))
    if match:
        return int(match.group(1)) * 10000
    return None

def estimate_company_difficulty(row):
    """ä¼æ¥­é›£æ˜“åº¦ã‚’æ¨å®š"""
    difficulty = 0.5

    avg_salary = extract_salary_numeric(row.get('avg_salary'))
    if avg_salary:
        if avg_salary >= 8000000:
            difficulty += 0.3
        elif avg_salary >= 6000000:
            difficulty += 0.2
        elif avg_salary >= 5000000:
            difficulty += 0.1

    employee_str = str(row.get('employee_count', ''))
    if '1ä¸‡äººä»¥ä¸Š' in employee_str:
        difficulty += 0.2

    return min(difficulty, 1.0)

def calculate_match_score(similarity_score, company_difficulty, industry_match, university_match=0.5):
    """ãƒãƒƒãƒã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = (
        similarity_score * 0.5 +
        (1 - company_difficulty) * 0.2 +
        industry_match * 0.2 +
        university_match * 0.1
    )
    return min(int(score * 100), 100)

def get_top_companies(similar_es, user_industry, user_university="", top_n=5):
    """TOPä¼æ¥­ã‚’é¸å‡º"""
    companies = []
    seen_companies = set()

    # ã¾ãšååˆ†ãªæ•°ã®ä¼æ¥­ã‚’å‡¦ç†ï¼ˆtop_nã®3å€ã¾ãŸã¯æœ€ä½20ç¤¾ï¼‰
    process_count = max(top_n * 3, 20)

    for _, row in similar_es.iterrows():
        company_name = row['company_name']

        if company_name in seen_companies:
            continue
        seen_companies.add(company_name)

        difficulty = estimate_company_difficulty(row)
        industry_match = 1.0 if user_industry in row['industry'] else 0.5
        university_match = 1.0 if user_university and user_university == row.get('university') else 0.5

        match_score = calculate_match_score(
            row['similarity_score'],
            difficulty,
            industry_match,
            university_match
        )

        reasons = []
        if row['similarity_score'] > 0.3:
            reasons.append('ESã®å†…å®¹ãŒé¡ä¼¼')
        if industry_match == 1.0:
            reasons.append('å¿—æœ›æ¥­ç•Œã¨ä¸€è‡´')
        if university_match == 1.0:
            reasons.append('åŒã˜å¤§å­¦ã‹ã‚‰ã®æ¡ç”¨å®Ÿç¸¾')
        if difficulty < 0.6:
            reasons.append('æ¯”è¼ƒçš„é€šéã—ã‚„ã™ã„')

        reason = 'ã€'.join(reasons) if reasons else 'ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒãƒ³ã‚°'

        salary = extract_salary_numeric(row['avg_salary'])
        if salary and salary >= 7000000:
            avg_gpa = "3.2-3.8"
        elif salary and salary >= 6000000:
            avg_gpa = "3.0-3.6"
        else:
            avg_gpa = "2.8-3.4"

        companies.append({
            'name': company_name,
            'industry': row['industry'],
            'matchScore': match_score,
            'reason': reason,
            'avgGpa': avg_gpa,
            'avgSalary': row.get('avg_salary', 'ä¸æ˜'),
            'employeeCount': row.get('employee_count', 'ä¸æ˜'),
        })

        # ååˆ†ãªæ•°ã®ä¼æ¥­ã‚’å‡¦ç†ã—ãŸã‚‰çµ‚äº†
        if len(companies) >= process_count:
            break

    # matchScoreã§é™é †ã‚½ãƒ¼ãƒˆã—ã¦top_nã‚’è¿”ã™
    companies.sort(key=lambda x: x['matchScore'], reverse=True)
    return companies[:top_n]

def calculate_target_company_match(company_name, similar_es, user_industry, user_university="", rank=1):
    """ç‰¹å®šã®å¿—æœ›ä¼æ¥­ã¨ã®ãƒãƒƒãƒç‡ã‚’è¨ˆç®—ï¼ˆå¿—æœ›é †ä½ã«å¿œã˜ã¦èª¿æ•´ï¼‰"""
    # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢
    company_data = es_data[es_data['company_name'] == company_name]

    if len(company_data) == 0:
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€é¡ä¼¼ESã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
        if len(similar_es) > 0:
            avg_score = similar_es['similarity_score'].mean()
            base_score = min(int(avg_score * 70), 100)  # æ§ãˆã‚ãªã‚¹ã‚³ã‚¢

            # å¿—æœ›é †ä½ã«ã‚ˆã‚‹èª¿æ•´
            rank_adjustment = 1.0 if rank == 1 else (0.95 if rank == 2 else 0.9)
            adjusted_score = int(base_score * rank_adjustment)

            return {
                'name': company_name,
                'industry': 'ä¸æ˜',
                'matchScore': adjusted_score,
                'reason': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚æ¨å®šå€¤ã§ã™',
                'dataCount': 0
            }
        return None

    # ä¼æ¥­ã®ä»£è¡¨çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    representative = company_data.iloc[0]

    # é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆãã®ä¼æ¥­ã®ESã¨ã®å¹³å‡é¡ä¼¼åº¦ï¼‰
    company_similarities = similar_es[similar_es['company_name'] == company_name]

    if len(company_similarities) > 0:
        avg_similarity = company_similarities['similarity_score'].mean()
    else:
        avg_similarity = similar_es['similarity_score'].mean() * 0.7  # æ§ãˆã‚ã«æ¨å®š

    difficulty = estimate_company_difficulty(representative)
    industry_match = 1.0 if user_industry in str(representative['industry']) else 0.5
    university_match = 1.0 if user_university and user_university == representative.get('university') else 0.5

    base_match_score = calculate_match_score(
        avg_similarity,
        difficulty,
        industry_match,
        university_match
    )

    # å¿—æœ›é †ä½ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆç¬¬ä¸€å¿—æœ›=100%ã€ç¬¬äºŒå¿—æœ›=95%ã€ç¬¬ä¸‰å¿—æœ›=90%ï¼‰
    rank_adjustment = 1.0 if rank == 1 else (0.95 if rank == 2 else 0.9)
    match_score = int(base_match_score * rank_adjustment)

    reasons = []
    if avg_similarity > 0.3:
        reasons.append('ESã®å†…å®¹ãŒé¡ä¼¼')
    if industry_match == 1.0:
        reasons.append('å¿—æœ›æ¥­ç•Œã¨ä¸€è‡´')
    if university_match == 1.0:
        reasons.append('åŒã˜å¤§å­¦ã‹ã‚‰ã®æ¡ç”¨å®Ÿç¸¾')
    if len(company_data) >= 10:
        reasons.append(f'{len(company_data)}ä»¶ã®åˆæ ¼ESå®Ÿç¸¾ã‚ã‚Š')
    elif len(company_data) >= 5:
        reasons.append(f'{len(company_data)}ä»¶ã®ESå®Ÿç¸¾ã‚ã‚Š')

    reason = 'ã€'.join(reasons) if reasons else 'ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒãƒ³ã‚°'

    return {
        'name': company_name,
        'industry': str(representative['industry']) if not pd.isna(representative['industry']) else 'ä¸æ˜',
        'matchScore': match_score,
        'reason': reason,
        'dataCount': len(company_data)
    }

def analyze_industry(industry):
    """æ¥­ç•Œåˆ†æ"""
    industry_data = es_data[es_data['industry'].str.contains(industry, na=False)]

    if len(industry_data) == 0:
        return {
            'passRate': 70,
            'avgApplicants': 150,
            'competition': 'ä¸­',
            'recommendations': ['æ¥­ç•Œç ”ç©¶ã‚’æ·±ã‚ã‚‹', 'ä¼æ¥­ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹']
        }

    pass_rate = 75
    avg_applicants = len(industry_data) * 3

    if avg_applicants > 200:
        competition = 'éå¸¸ã«é«˜'
    elif avg_applicants > 150:
        competition = 'é«˜'
    elif avg_applicants > 100:
        competition = 'ä¸­'
    else:
        competition = 'ä½'

    recommendations_map = {
        'IT': ['æŠ€è¡“ã‚¹ã‚­ãƒ«ã®è¨¼æ˜', 'ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ä½œæˆ', 'æœ€æ–°æŠ€è¡“ã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—'],
        'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°': ['ã‚±ãƒ¼ã‚¹é¢æ¥å¯¾ç­–', 'è«–ç†çš„æ€è€ƒåŠ›ã®å¼·åŒ–', 'ãƒ•ã‚§ãƒ«ãƒŸæ¨å®šã®ç·´ç¿’'],
        'é‡‘è': ['é‡‘èçŸ¥è­˜ã®ç¿’å¾—', 'æ•°å­—ã«å¼·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰', 'èª å®Ÿã•ã®ã‚¢ãƒ”ãƒ¼ãƒ«'],
        'ãƒ¡ãƒ¼ã‚«ãƒ¼': ['æŠ€è¡“åŠ›ã®è¨¼æ˜', 'é•·æœŸçš„ãªã‚­ãƒ£ãƒªã‚¢ãƒ“ã‚¸ãƒ§ãƒ³', 'ã‚‚ã®ã¥ãã‚Šã¸ã®æƒ…ç†±']
    }

    recommendations = recommendations_map.get(
        industry,
        ['æ¥­ç•Œç ”ç©¶ã‚’æ·±ã‚ã‚‹', 'ä¼æ¥­ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹', 'è‡ªå·±åˆ†æã‚’å¾¹åº•ã™ã‚‹']
    )

    return {
        'passRate': pass_rate,
        'avgApplicants': avg_applicants,
        'competition': competition,
        'recommendations': recommendations
    }

def analyze_gakuchika(gakuchika_text):
    """ã‚¬ã‚¯ãƒã‚«åˆ†æ"""
    strengths = []
    improvements = []

    if any(word in gakuchika_text for word in ['æ•°å€¤', 'çµæœ', 'æˆæœ', '%', 'äºº']):
        strengths.append('å…·ä½“çš„ãªæ•°å€¤ãƒ»æˆæœã®è¨˜è¼‰')

    if any(word in gakuchika_text for word in ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„']):
        strengths.append('èª²é¡Œè§£æ±ºã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜ç¢º')

    if len(gakuchika_text) >= 300:
        strengths.append('ååˆ†ãªåˆ†é‡ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹')

    if not any(word in gakuchika_text for word in ['ãƒãƒ¼ãƒ ', 'å”åŠ›', 'é€£æº', 'ãƒ¡ãƒ³ãƒãƒ¼']):
        improvements.append('ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¦ç´ ã‚’è¿½åŠ ')

    if not any(word in gakuchika_text for word in ['å­¦ã‚“ã ', 'å¾—ãŸ', 'æˆé•·']):
        improvements.append('å­¦ã³ã‚„æˆé•·ã®è¦ç´ ã‚’å¼·èª¿')

    if len(gakuchika_text) < 200:
        improvements.append('ã‚‚ã†å°‘ã—è©³ã—ãè¨˜è¿°ã™ã‚‹')

    return {
        'strengths': strengths if strengths else ['åŸºæœ¬çš„ãªæ§‹æˆã¯è‰¯å¥½'],
        'improvements': improvements if improvements else ['ç¾çŠ¶ã§è‰¯ã„å†…å®¹ã§ã™']
    }

def analyze_es_answers(answers):
    """ESåˆ†æï¼ˆè¤‡æ•°å›ç­”å¯¾å¿œï¼‰"""
    all_text = ' '.join([a for a in answers if a])

    strengths = []
    improvements = []

    if any(word in all_text for word in ['æ•°å€¤', 'çµæœ', 'æˆæœ', '%', 'äºº', 'ä»¶', 'å€']):
        strengths.append('å…·ä½“çš„ãªæ•°å€¤ãƒ»æˆæœã®è¨˜è¼‰')
    if any(word in all_text for word in ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å…‹æœ']):
        strengths.append('èª²é¡Œè§£æ±ºã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜ç¢º')
    if any(word in all_text for word in ['ãƒãƒ¼ãƒ ', 'å”åŠ›', 'é€£æº', 'ãƒ¡ãƒ³ãƒãƒ¼', 'çµ„ç¹”']):
        strengths.append('ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¦ç´ ãŒã‚ã‚‹')
    if len(all_text) >= 500:
        strengths.append('ååˆ†ãªåˆ†é‡ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹')

    if not any(word in all_text for word in ['å­¦ã‚“ã ', 'å¾—ãŸ', 'æˆé•·', 'çµŒé¨“']):
        improvements.append('å­¦ã³ã‚„æˆé•·ã®è¦ç´ ã‚’å¼·èª¿')
    if not any(word in all_text for word in ['å…·ä½“çš„', 'ä¾‹ãˆã°', 'å®Ÿéš›ã«']):
        improvements.append('ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ')
    if len(all_text) < 300:
        improvements.append('ã‚‚ã†å°‘ã—è©³ã—ãè¨˜è¿°ã™ã‚‹')

    return {
        'strengths': strengths if strengths else ['åŸºæœ¬çš„ãªæ§‹æˆã¯è‰¯å¥½'],
        'improvements': improvements if improvements else ['ç¾çŠ¶ã§è‰¯ã„å†…å®¹ã§ã™']
    }

def get_industry_similar_es_samples(similar_es, target_industry, top_n=3):
    """å¿—æœ›æ¥­ç•Œå†…ã®é¡ä¼¼åº¦ã®é«˜ã„ESã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—

    ã¾ãšå°åˆ†é¡ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã§æ¤œç´¢ã—ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å¤§åˆ†é¡ã§æ¤œç´¢ã™ã‚‹
    """
    # 1. ã¾ãšå°åˆ†é¡ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ã§æ¤œç´¢
    industry_es = similar_es[similar_es['industry'].str.contains(target_industry, na=False)]
    exact_match = True
    matched_category = target_industry

    # 2. å°åˆ†é¡ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å¤§åˆ†é¡ã§æ¤œç´¢
    if len(industry_es) == 0:
        major_category = extract_major_industry_category(target_industry)
        if major_category:
            # å¤§åˆ†é¡ã§å§‹ã¾ã‚‹æ¥­ç•Œã‚’ã™ã¹ã¦æ¤œç´¢
            industry_es = similar_es[similar_es['industry'].str.startswith(major_category, na=False)]
            exact_match = False
            matched_category = major_category

        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        if len(industry_es) == 0:
            return {'samples': [], 'exactMatch': False, 'matchedCategory': None}

    samples = []

    for idx, row in industry_es.head(top_n).iterrows():
        user_info = str(row.get('user_info', ''))

        # å’æ¥­å¹´åº¦ã‚’æŠ½å‡º
        grad_year_match = re.search(r'(\d{2})å’', user_info)
        grad_year = grad_year_match.group(1) + 'å’' if grad_year_match else 'ä¸æ˜'

        university = row.get('university', 'ä¸æ˜')

        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚’æŠ½å‡º
        major_match = re.search(r'\|\s*([^|]+)\s*\|', user_info)
        major = major_match.group(1).strip() if major_match else 'ä¸æ˜'

        es_content = []
        for i in range(1, 4):
            question = row.get(f'question_{i}', '')
            answer = row.get(f'answer_{i}', '')

            if question and answer and str(question).strip() and str(answer).strip():
                es_content.append({
                    'question': str(question).strip(),
                    'answer': str(answer).strip()[:500] + ('...' if len(str(answer)) > 500 else '')
                })

        if len(es_content) > 0:
            sample = {
                'company': str(row['company_name']),
                'industry': str(row['industry']) if not pd.isna(row['industry']) else 'ä¸æ˜',
                'result': str(row['result_status']),
                'similarity': round(float(row['similarity_score']) * 100, 1),
                'profile': {
                    'university': university,
                    'major': major,
                    'gradYear': grad_year
                },
                'esContent': es_content
            }
            samples.append(sample)

    return {
        'samples': samples,
        'exactMatch': exact_match,
        'matchedCategory': matched_category
    }

def get_similar_es_samples(similar_es, top_n=3):
    """é¡ä¼¼ESã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
    samples = []

    for idx, row in similar_es.head(top_n).iterrows():
        user_info = str(row.get('user_info', ''))

        # å’æ¥­å¹´åº¦ã‚’æŠ½å‡º
        grad_year_match = re.search(r'(\d{2})å’', user_info)
        grad_year = grad_year_match.group(1) + 'å’' if grad_year_match else 'ä¸æ˜'

        university = row.get('university', 'ä¸æ˜')

        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚’æŠ½å‡º
        major_match = re.search(r'\|\s*([^|]+)\s*\|', user_info)
        major = major_match.group(1).strip() if major_match else 'ä¸æ˜'

        es_content = []
        for i in range(1, 4):
            question = row.get(f'question_{i}', '')
            answer = row.get(f'answer_{i}', '')

            if question and answer and str(question).strip() and str(answer).strip():
                es_content.append({
                    'question': str(question).strip(),
                    'answer': str(answer).strip()[:500] + ('...' if len(str(answer)) > 500 else '')
                })

        if len(es_content) > 0:
            sample = {
                'company': str(row['company_name']),
                'industry': str(row['industry']) if not pd.isna(row['industry']) else 'ä¸æ˜',
                'result': str(row['result_status']),
                'similarity': round(float(row['similarity_score']) * 100, 1),
                'profile': {
                    'university': university,
                    'major': major,
                    'gradYear': grad_year
                },
                'esContent': es_content
            }
            samples.append(sample)

    return samples

def get_es_samples_by_company(similar_es, company_name, top_n=3):
    """
    æŒ‡å®šã—ãŸä¼æ¥­ã®é¡ä¼¼ESã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—

    Args:
        similar_es: é¡ä¼¼åº¦è¨ˆç®—æ¸ˆã¿ã®ES DataFrame
        company_name: ä¼æ¥­å
        top_n: è¿”ã™ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
        list: ESã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    # ã“ã®ä¼æ¥­ã®ESã‚’é¡ä¼¼åº¦é †ã§å–å¾—
    company_es = similar_es[similar_es['company_name'] == company_name]

    if len(company_es) == 0:
        return []

    samples = []

    for idx, row in company_es.head(top_n).iterrows():
        user_info = str(row.get('user_info', ''))

        # å’æ¥­å¹´åº¦ã‚’æŠ½å‡º
        grad_year_match = re.search(r'(\d{2})å’', user_info)
        grad_year = grad_year_match.group(1) + 'å’' if grad_year_match else 'ä¸æ˜'

        university = row.get('university', 'ä¸æ˜')

        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚’æŠ½å‡º
        major_match = re.search(r'\|\s*([^|]+)\s*\|', user_info)
        major = major_match.group(1).strip() if major_match else 'ä¸æ˜'

        es_content = []
        for i in range(1, 4):
            question = row.get(f'question_{i}', '')
            answer = row.get(f'answer_{i}', '')

            if question and answer and str(question).strip() and str(answer).strip():
                es_content.append({
                    'question': str(question).strip(),
                    'answer': str(answer).strip()[:500] + ('...' if len(str(answer)) > 500 else '')
                })

        if len(es_content) > 0:
            sample = {
                'company': str(row['company_name']),
                'industry': str(row['industry']) if not pd.isna(row['industry']) else 'ä¸æ˜',
                'result': str(row['result_status']),
                'similarity': round(float(row['similarity_score']) * 100, 1),
                'profile': {
                    'university': university,
                    'major': major,
                    'gradYear': grad_year
                },
                'esContent': es_content
            }
            samples.append(sample)

    return samples

def get_similar_es_samples_from_top_companies(similar_es, top_companies):
    """
    TOPä¼æ¥­ãƒªã‚¹ãƒˆã«åŸºã¥ã„ã¦é¡ä¼¼ESã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—ï¼ˆé †åºã‚’ç¶­æŒï¼‰
    â€»ã“ã®é–¢æ•°ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã¾ã™ãŒã€éæ¨å¥¨ã§ã™

    Args:
        similar_es: é¡ä¼¼åº¦è¨ˆç®—æ¸ˆã¿ã®ES DataFrame
        top_companies: get_top_companies()ã‹ã‚‰è¿”ã•ã‚ŒãŸä¼æ¥­ãƒªã‚¹ãƒˆ

    Returns:
        list: TOPä¼æ¥­ã¨åŒã˜é †åºãƒ»åŒã˜ä¼æ¥­ã®ESã‚µãƒ³ãƒ—ãƒ«
    """
    samples = []

    for company_info in top_companies:
        company_name = company_info['name']
        company_samples = get_es_samples_by_company(similar_es, company_name, top_n=1)

        if len(company_samples) > 0:
            # matchScoreã‚’è¿½åŠ 
            company_samples[0]['matchScore'] = company_info['matchScore']
            samples.append(company_samples[0])

    return samples


def get_episode_type_similar_es_samples(similar_es, input_text, top_n=3):
    """
    åŒã˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®é¡ä¼¼ESã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—

    Args:
        similar_es: é¡ä¼¼åº¦è¨ˆç®—æ¸ˆã¿ã®ES DataFrame
        input_text: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ESæœ¬æ–‡
        top_n: è¿”ã™ã‚µãƒ³ãƒ—ãƒ«æ•°

    Returns:
        dict: {
            'episodeType': ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—å,
            'episodeTypeJa': ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªå,
            'confidence': ä¿¡é ¼åº¦,
            'samples': [é¡ä¼¼ESã®ãƒªã‚¹ãƒˆ],
            'totalCount': åŒã˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®ESç·æ•°
        }
    """
    # å…¥åŠ›ESã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
    input_episode_info = classify_episode_type(input_text)
    input_episode_type = input_episode_info['type']
    input_confidence = input_episode_info['confidence']

    print(f"  ğŸ¯ å…¥åŠ›ESã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—: {input_episode_type} (ä¿¡é ¼åº¦: {input_confidence})")

    # åŒã˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®ESã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    same_episode_es = similar_es[
        similar_es['episode_type'].apply(lambda x: x['type'] == input_episode_type)
    ]

    # ä»¶æ•°ãŒå°‘ãªã„å ´åˆã¯ã€ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã§ã‚‚æ¤œç´¢
    if len(same_episode_es) < top_n:
        print(f"  âš ï¸ åŒä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®ESãŒ{len(same_episode_es)}ä»¶ã®ã¿ã€‚ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã§è¿½åŠ æ¤œç´¢...")

        # ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«ã§åŒã˜ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’å«ã‚€ESã‚’è¿½åŠ 
        multi_episode_es = similar_es[
            similar_es['episode_types_multi'].apply(
                lambda types: any(t['type'] == input_episode_type for t in types)
            )
        ]

        # é‡è¤‡ã‚’é™¤å¤–ã—ã¦çµåˆ
        same_episode_es = pd.concat([same_episode_es, multi_episode_es]).drop_duplicates()

    total_count = len(same_episode_es)

    if total_count == 0:
        return {
            'episodeType': input_episode_type,
            'episodeTypeJa': input_episode_type,
            'confidence': input_confidence,
            'samples': [],
            'totalCount': 0,
            'message': f'ã€Œ{input_episode_type}ã€ã‚«ãƒ†ã‚´ãƒªã®ESãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
        }

    # ä¸Šä½top_nã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
    samples = []

    for idx, row in same_episode_es.head(top_n).iterrows():
        user_info = str(row.get('user_info', ''))

        # å’æ¥­å¹´åº¦ã‚’æŠ½å‡º
        grad_year_match = re.search(r'(\d{2})å’', user_info)
        grad_year = grad_year_match.group(1) + 'å’' if grad_year_match else 'ä¸æ˜'

        university = row.get('university', 'ä¸æ˜')

        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚’æŠ½å‡º
        major_match = re.search(r'\|\s*([^|]+)\s*\|', user_info)
        major = major_match.group(1).strip() if major_match else 'ä¸æ˜'

        es_content = []
        for i in range(1, 4):
            question = row.get(f'question_{i}', '')
            answer = row.get(f'answer_{i}', '')

            if question and answer and str(question).strip() and str(answer).strip():
                es_content.append({
                    'question': str(question).strip(),
                    'answer': str(answer).strip()[:500] + ('...' if len(str(answer)) > 500 else '')
                })

        if len(es_content) > 0:
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—
            episode_info = row['episode_type']

            sample = {
                'company': str(row['company_name']),
                'industry': str(row['industry']) if not pd.isna(row['industry']) else 'ä¸æ˜',
                'result': str(row['result_status']),
                'similarity': round(float(row['similarity_score']) * 100, 1),
                'episodeType': episode_info['type'],
                'episodeConfidence': episode_info['confidence'],
                'profile': {
                    'university': university,
                    'major': major,
                    'gradYear': grad_year
                },
                'esContent': es_content
            }
            samples.append(sample)

    return {
        'episodeType': input_episode_type,
        'episodeTypeJa': input_episode_type,  # æ—¥æœ¬èªåï¼ˆæ—¢ã«æ—¥æœ¬èªï¼‰
        'confidence': input_confidence,
        'samples': samples,
        'totalCount': total_count,
        'message': f'åŒã˜ã€Œ{input_episode_type}ã€ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰{len(samples)}ä»¶ã®ESã‚’æŠ½å‡ºã—ã¾ã—ãŸ'
    }

# ============================================
# HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆç¾ã—ã„UIï¼‰
# ============================================

# HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ templates/index.html ã‹ã‚‰èª­ã¿è¾¼ã¿

# ============================================
# Flaskãƒ«ãƒ¼ãƒˆ
# ============================================

@app.route('/')
def home():
    """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰UI - ãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã‚“ã HTMLã‚’è¿”ã™"""
    print("\nğŸŒ ãƒšãƒ¼ã‚¸ç”Ÿæˆä¸­...")

    # ä¼æ¥­ã¨æ¥­ç•Œã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    company_industries = {}
    for company in companies_list[:300]:
        company_data = es_data[es_data['company_name'] == company]
        if len(company_data) > 0:
            # æœ€ã‚‚å¤šã„æ¥­ç•Œã‚’å–å¾—
            industry = company_data['industry'].mode()[0] if len(company_data['industry'].mode()) > 0 else 'ä¸æ˜'
            company_industries[company] = industry

    # é¸æŠè‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    embedded_data = {
        'universities': universities_list[:200],  # æœ€åˆã®200æ ¡
        'industries': industries_list,
        'companies': companies_list[:300],  # æœ€åˆã®300ç¤¾
        'commonQuestions': common_questions,
        'companyCounts': {k: v for k, v in company_counts.items() if k in companies_list[:300]},
        'industryCounts': industry_counts,
        'companyIndustries': company_industries  # ä¼æ¥­ã¨æ¥­ç•Œã®ãƒãƒƒãƒ”ãƒ³ã‚°
    }

    # JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºï¼ˆensure_ascii=Trueã§å®‰å…¨ã«ï¼‰
    embedded_data_json = json.dumps(embedded_data, ensure_ascii=True)

    # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template_path = os.path.join(template_dir, 'index.html')
    with open(template_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    # <script type="application/json">ã§åŸ‹ã‚è¾¼ã¿
    embedded_script = f"""
    <script id="embedded-data" type="application/json">
{embedded_data_json}
    </script>
    """

    # HTMLã®</head>ç›´å‰ã«æŒ¿å…¥
    html_content = html_content.replace('</head>', embedded_script + '\n</head>')

    print(f"  âœ… ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿å®Œäº†: å¤§å­¦{len(embedded_data['universities'])}æ ¡, æ¥­ç•Œ{len(embedded_data['industries'])}ç¨®é¡")

    return Response(html_content, mimetype='text/html')

@app.route('/analyze', methods=['POST'])
def analyze_es():
    """ESè¨ºæ–­API - è¤‡æ•°ESè³ªå•å¯¾å¿œ"""
    try:
        data = request.get_json()

        # è¤‡æ•°ã®ESå›ç­”ã«å¯¾å¿œ
        if not data.get('esAnswers') or len(data.get('esAnswers', [])) == 0:
            return jsonify({'error': 'ESå›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„'}), 400

        has_long_answer = any(len(ans) >= 100 for ans in data['esAnswers'])
        if not has_long_answer:
            return jsonify({'error': 'å°‘ãªãã¨ã‚‚1ã¤ã®å›ç­”ã¯100æ–‡å­—ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„'}), 400

        if not data.get('targetIndustry'):
            return jsonify({'error': 'å¿—æœ›æ¥­ç•Œã‚’é¸æŠã—ã¦ãã ã•ã„'}), 400

        # å…¨ã¦ã®å›ç­”ã‚’çµåˆã—ã¦é¡ä¼¼åº¦è¨ˆç®—
        combined_answers = ' '.join(data['esAnswers'])
        similar_es = calculate_similarity(combined_answers, top_n=100)

        top_companies = get_top_companies(
            similar_es,
            data['targetIndustry'],
            data.get('university', ''),
            top_n=5
        )

        # å„TOPä¼æ¥­ã«ESã‚µãƒ³ãƒ—ãƒ«ã‚’è¿½åŠ ï¼ˆã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ç”¨ï¼‰
        for company in top_companies:
            company['esSamples'] = get_es_samples_by_company(
                similar_es,
                company['name'],
                top_n=3  # å„ä¼æ¥­ã‹ã‚‰3ä»¶ã®ESã‚’å–å¾—
            )

        industry_analysis = analyze_industry(data['targetIndustry'])
        es_analysis = analyze_es_answers(data['esAnswers'])
        industry_similar_es_samples = get_industry_similar_es_samples(similar_es, data['targetIndustry'], top_n=3)

        # å…¥åŠ›ESã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        input_episode_info = classify_episode_type(combined_answers)
        input_episode_types_multi = classify_multiple_episode_types(combined_answers, top_n=2)

        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®é¡ä¼¼ES
        episode_type_similar_es_samples = get_episode_type_similar_es_samples(
            similar_es,
            combined_answers,
            top_n=3
        )

        # å¿—æœ›ä¼æ¥­ã®ãƒãƒƒãƒç‡ã‚’è¨ˆç®—ï¼ˆç¬¬ä¸‰å¿—æœ›ã¾ã§ï¼‰
        target_companies_match = []
        if data.get('targetCompanies') and len(data['targetCompanies']) > 0:
            for i, target_company in enumerate(data['targetCompanies'], 1):
                if target_company and target_company.strip():
                    match_result = calculate_target_company_match(
                        target_company,
                        similar_es,
                        data['targetIndustry'],
                        data.get('university', ''),
                        rank=i  # å¿—æœ›é †ä½ã‚’æ¸¡ã™
                    )
                    if match_result:
                        # å¿—æœ›é †ä½ã‚’è¿½åŠ 
                        match_result['rank'] = i
                        # ESã‚µãƒ³ãƒ—ãƒ«ã‚’è¿½åŠ ï¼ˆã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ç”¨ï¼‰
                        match_result['esSamples'] = get_es_samples_by_company(
                            similar_es,
                            target_company,
                            top_n=3  # å„ä¼æ¥­ã‹ã‚‰3ä»¶ã®ESã‚’å–å¾—
                        )
                        target_companies_match.append(match_result)

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_es_count = len(es_data)
        matched_es_count = len(similar_es)
        industry_es_count = len(es_data[es_data['industry'].str.contains(data['targetIndustry'], na=False)])

        # å¿—æœ›ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        target_companies_data_count = {}
        if data.get('targetCompanies') and len(data['targetCompanies']) > 0:
            for target_company in data['targetCompanies']:
                if target_company and target_company.strip():
                    count = len(es_data[es_data['company_name'] == target_company])
                    target_companies_data_count[target_company] = count

        # ç¬¬ä¸‰å¿—æœ›ã¾ã§ã®ãƒãƒƒãƒç‡ã®å¹³å‡ã‚’è¨ˆç®—
        avg_match_rate = 0
        if len(target_companies_match) > 0:
            avg_match_rate = sum(item['matchScore'] for item in target_companies_match) / len(target_companies_match)

        response = {
            'matchCompanies': top_companies,  # å„ä¼æ¥­ã«esSamplesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ æ¸ˆã¿ï¼ˆã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ç”¨ï¼‰
            'industryAnalysis': industry_analysis,
            'esAnalysis': es_analysis,
            'industrySimilarESSamples': industry_similar_es_samples,  # æ¥­ç•Œå†…ã®é¡ä¼¼ES
            'episodeTypeSimilarESSamples': episode_type_similar_es_samples,  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®é¡ä¼¼ES
            'targetCompaniesMatch': target_companies_match,  # å„ä¼æ¥­ã«esSamplesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ æ¸ˆã¿ï¼ˆã‚¢ã‚³ãƒ¼ãƒ‡ã‚£ã‚ªãƒ³ç”¨ï¼‰
            'dataStatistics': {
                'totalEsCount': total_es_count,
                'matchedEsCount': matched_es_count,
                'industryEsCount': industry_es_count,
                'targetCompaniesDataCount': target_companies_data_count,
                'avgMatchRate': round(avg_match_rate, 1)
            },
            'userInfo': {
                'university': data.get('university'),
                'major': data.get('major'),
                'graduationYear': data.get('graduationYear')
            },
            'episodeTypeInfo': {  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—æƒ…å ±
                'primary': input_episode_info,
                'all': input_episode_types_multi
            }
        }

        return jsonify(response)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

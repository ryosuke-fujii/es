# ============================================
# ã‚»ãƒ«3: ESè¨ºæ–­ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆFlaskçµ±åˆç‰ˆï¼‰
# ============================================
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from flask import Flask, request, jsonify, render_template, Response
import json
import re
import threading
import time
import os

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
es_data = None
vectorizer = None
tfidf_matrix = None

# é¸æŠè‚¢ç”¨ãƒ‡ãƒ¼ã‚¿
universities_list = []
industries_list = []
companies_list = []
common_questions = []
company_counts = {}
industry_counts = {}

# Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
# templatesãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã‚€
base_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(os.path.dirname(base_dir), 'templates')
app = Flask(__name__, template_folder=template_dir)

# ============================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
# ============================================

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if pd.isna(text):
        return ""
    text = re.sub(r'\n+', ' ', str(text))
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'ç¶šãã‚’èª­ã‚€.*', '', text)
    text = re.sub(r'å•é¡Œã‚’å ±å‘Šã™ã‚‹.*', '', text)
    return text.strip()

def remove_prefix(text):
    """ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤"""
    if pd.isna(text):
        return ""
    return re.sub(r'^[^ï¼š]+ï¼š\s*', '', str(text))

def extract_university(user_info):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‹ã‚‰å¤§å­¦åã‚’æŠ½å‡º"""
    if pd.isna(user_info):
        return "ä¸æ˜"
    match = re.search(r'\d{2}å’\s*\|\s*([^|]+)\s*\|', str(user_info))
    if match:
        return match.group(1).strip()
    return "ä¸æ˜"

def load_csv_data(csv_path):
    """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ•´å½¢"""
    global es_data, vectorizer, tfidf_matrix

    print(f"\nğŸ“‚ CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­...")

    es_data = pd.DataFrame({
        'company_name': df['p-company-summary__name'].apply(clean_text),
        'industry': df['p-company-summary__stage-sub'].apply(remove_prefix),
        'title': df['p-company-heading-contents__title'].apply(clean_text),
        'user_info': df.get('c-panel-variant2__header-user', pd.Series()).apply(clean_text),
        'question_1': df['u-font-light'].apply(clean_text),
        'answer_1': df['c-show-more__content'].apply(clean_text),
        'question_2': df.get('u-font-light (2)', pd.Series()).apply(clean_text),
        'answer_2': df.get('c-show-more__content (2)', pd.Series()).apply(clean_text),
        'question_3': df.get('u-font-light (3)', pd.Series()).apply(clean_text),
        'answer_3': df.get('c-show-more__content (3)', pd.Series()).apply(clean_text),
        'avg_salary': df.get('p-company-table (11)', pd.Series()).apply(clean_text),
        'employee_count': df.get('p-company-summary__stage-sub (3)', pd.Series()).apply(remove_prefix),
    })

    es_data['university'] = es_data['user_info'].apply(extract_university)

    es_data['result_status'] = es_data['title'].apply(
        lambda x: 'å†…å®š' if 'å†…å®š' in str(x) else ('é€šé' if 'é€šé' in str(x) else 'ä¸æ˜')
    )

    es_data = es_data[es_data['result_status'].isin(['é€šé', 'å†…å®š'])]

    es_data['combined_answer'] = (
        es_data['answer_1'].fillna('') + ' ' +
        es_data['answer_2'].fillna('') + ' ' +
        es_data['answer_3'].fillna('')
    ).str.strip()

    es_data = es_data[es_data['combined_answer'].str.len() > 50]

    print(f"âœ… æœ‰åŠ¹ãªESãƒ‡ãƒ¼ã‚¿: {len(es_data)}ä»¶")

    print("ğŸ”§ TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
    vectorizer = TfidfVectorizer(
        max_features=1000,
        min_df=2,
        max_df=0.8,
        ngram_range=(1, 2)
    )

    tfidf_matrix = vectorizer.fit_transform(es_data['combined_answer'])
    print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: {tfidf_matrix.shape}")

    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ä¼æ¥­æ•°: {es_data['company_name'].nunique()}")
    print(f"  - æ¥­ç•Œæ•°: {es_data['industry'].nunique()}")
    print(f"  - é€šéES: {(es_data['result_status'] == 'é€šé').sum()}ä»¶")
    print(f"  - å†…å®šES: {(es_data['result_status'] == 'å†…å®š').sum()}ä»¶")

    # é¸æŠè‚¢ã‚’æŠ½å‡º
    global universities_list, industries_list, companies_list, common_questions
    global company_counts, industry_counts

    print("\nğŸ“‹ é¸æŠè‚¢ã‚’æŠ½å‡ºä¸­...")

    universities_list = sorted(es_data['university'].dropna().unique().tolist())
    universities_list = [u for u in universities_list if u != "ä¸æ˜" and str(u).strip() != ""]

    industries_list = sorted(es_data['industry'].dropna().unique().tolist())
    industries_list = [i for i in industries_list if i and str(i).strip() != ""]

    companies_list = sorted(es_data['company_name'].dropna().unique().tolist())
    companies_list = [c for c in companies_list if c and str(c).strip() != ""]

    # ä¼æ¥­ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    company_counts = es_data['company_name'].value_counts().to_dict()

    # æ¥­ç•Œã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    industry_counts = es_data['industry'].value_counts().to_dict()

    common_questions = [
        "å­¦ç”Ÿæ™‚ä»£ã«åŠ›ã‚’å…¥ã‚ŒãŸã“ã¨ï¼ˆã‚¬ã‚¯ãƒã‚«ï¼‰",
        "å¿—æœ›å‹•æ©Ÿ",
        "è‡ªå·±PR",
        "ã‚ãªãŸã®å¼·ã¿ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰",
        "æŒ‘æˆ¦ã—ãŸã“ã¨ãƒ»ãƒãƒ£ãƒ¬ãƒ³ã‚¸",
        "å›°é›£ã‚’ä¹—ã‚Šè¶ŠãˆãŸçµŒé¨“",
        "ãƒãƒ¼ãƒ ã§æˆæœã‚’å‡ºã—ãŸçµŒé¨“",
        "ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚’ç™ºæ®ã—ãŸçµŒé¨“",
        "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§å­¦ã³ãŸã„ã“ã¨",
        "å°†æ¥ã®ã‚­ãƒ£ãƒªã‚¢ãƒ“ã‚¸ãƒ§ãƒ³"
    ]

    print(f"âœ… é¸æŠè‚¢ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"  - å¤§å­¦: {len(universities_list)}æ ¡")
    print(f"  - æ¥­ç•Œ: {len(industries_list)}ç¨®é¡")
    print(f"  - ä¼æ¥­: {len(companies_list)}ç¤¾")

def calculate_similarity(input_text, top_n=100):
    """é¡ä¼¼åº¦è¨ˆç®—"""
    input_vector = vectorizer.transform([input_text])
    similarities = cosine_similarity(input_vector, tfidf_matrix)[0]

    result = es_data.copy()
    result['similarity_score'] = similarities
    result = result.sort_values('similarity_score', ascending=False).head(top_n)

    return result

def extract_salary_numeric(salary_str):
    """çµ¦ä¸ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if pd.isna(salary_str):
        return None
    match = re.search(r'(\d+)ä¸‡', str(salary_str))
    if match:
        return int(match.group(1)) * 10000
    return None

def estimate_company_difficulty(row):
    """ä¼æ¥­é›£æ˜“åº¦ã‚’æ¨å®š"""
    difficulty = 0.5

    avg_salary = extract_salary_numeric(row.get('avg_salary'))
    if avg_salary:
        if avg_salary >= 8000000:
            difficulty += 0.3
        elif avg_salary >= 6000000:
            difficulty += 0.2
        elif avg_salary >= 5000000:
            difficulty += 0.1

    employee_str = str(row.get('employee_count', ''))
    if '1ä¸‡äººä»¥ä¸Š' in employee_str:
        difficulty += 0.2

    return min(difficulty, 1.0)

def calculate_match_score(similarity_score, company_difficulty, industry_match, university_match=0.5):
    """ãƒãƒƒãƒã‚¹ã‚³ã‚¢è¨ˆç®—"""
    score = (
        similarity_score * 0.5 +
        (1 - company_difficulty) * 0.2 +
        industry_match * 0.2 +
        university_match * 0.1
    )
    return min(int(score * 100), 100)

def get_top_companies(similar_es, user_industry, user_university="", top_n=5):
    """TOPä¼æ¥­ã‚’é¸å‡º"""
    companies = []
    seen_companies = set()

    for _, row in similar_es.iterrows():
        company_name = row['company_name']

        if company_name in seen_companies:
            continue
        seen_companies.add(company_name)

        difficulty = estimate_company_difficulty(row)
        industry_match = 1.0 if user_industry in row['industry'] else 0.5
        university_match = 1.0 if user_university and user_university == row.get('university') else 0.5

        match_score = calculate_match_score(
            row['similarity_score'],
            difficulty,
            industry_match,
            university_match
        )

        reasons = []
        if row['similarity_score'] > 0.3:
            reasons.append('ESã®å†…å®¹ãŒé¡ä¼¼')
        if industry_match == 1.0:
            reasons.append('å¿—æœ›æ¥­ç•Œã¨ä¸€è‡´')
        if university_match == 1.0:
            reasons.append('åŒã˜å¤§å­¦ã‹ã‚‰ã®æ¡ç”¨å®Ÿç¸¾')
        if difficulty < 0.6:
            reasons.append('æ¯”è¼ƒçš„é€šéã—ã‚„ã™ã„')

        reason = 'ã€'.join(reasons) if reasons else 'ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒãƒ³ã‚°'

        salary = extract_salary_numeric(row['avg_salary'])
        if salary and salary >= 7000000:
            avg_gpa = "3.2-3.8"
        elif salary and salary >= 6000000:
            avg_gpa = "3.0-3.6"
        else:
            avg_gpa = "2.8-3.4"

        companies.append({
            'name': company_name,
            'industry': row['industry'],
            'matchScore': match_score,
            'reason': reason,
            'avgGpa': avg_gpa,
            'avgSalary': row.get('avg_salary', 'ä¸æ˜'),
            'employeeCount': row.get('employee_count', 'ä¸æ˜'),
        })

        if len(companies) >= top_n:
            break

    return companies

def calculate_target_company_match(company_name, similar_es, user_industry, user_university="", rank=1):
    """ç‰¹å®šã®å¿—æœ›ä¼æ¥­ã¨ã®ãƒãƒƒãƒç‡ã‚’è¨ˆç®—ï¼ˆå¿—æœ›é †ä½ã«å¿œã˜ã¦èª¿æ•´ï¼‰"""
    # ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢
    company_data = es_data[es_data['company_name'] == company_name]

    if len(company_data) == 0:
        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€é¡ä¼¼ESã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
        if len(similar_es) > 0:
            avg_score = similar_es['similarity_score'].mean()
            base_score = min(int(avg_score * 70), 100)  # æ§ãˆã‚ãªã‚¹ã‚³ã‚¢

            # å¿—æœ›é †ä½ã«ã‚ˆã‚‹èª¿æ•´
            rank_adjustment = 1.0 if rank == 1 else (0.95 if rank == 2 else 0.9)
            adjusted_score = int(base_score * rank_adjustment)

            return {
                'name': company_name,
                'industry': 'ä¸æ˜',
                'matchScore': adjusted_score,
                'reason': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚æ¨å®šå€¤ã§ã™',
                'dataCount': 0
            }
        return None

    # ä¼æ¥­ã®ä»£è¡¨çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    representative = company_data.iloc[0]

    # é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆãã®ä¼æ¥­ã®ESã¨ã®å¹³å‡é¡ä¼¼åº¦ï¼‰
    company_similarities = similar_es[similar_es['company_name'] == company_name]

    if len(company_similarities) > 0:
        avg_similarity = company_similarities['similarity_score'].mean()
    else:
        avg_similarity = similar_es['similarity_score'].mean() * 0.7  # æ§ãˆã‚ã«æ¨å®š

    difficulty = estimate_company_difficulty(representative)
    industry_match = 1.0 if user_industry in str(representative['industry']) else 0.5
    university_match = 1.0 if user_university and user_university == representative.get('university') else 0.5

    base_match_score = calculate_match_score(
        avg_similarity,
        difficulty,
        industry_match,
        university_match
    )

    # å¿—æœ›é †ä½ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆç¬¬ä¸€å¿—æœ›=100%ã€ç¬¬äºŒå¿—æœ›=95%ã€ç¬¬ä¸‰å¿—æœ›=90%ï¼‰
    rank_adjustment = 1.0 if rank == 1 else (0.95 if rank == 2 else 0.9)
    match_score = int(base_match_score * rank_adjustment)

    reasons = []
    if avg_similarity > 0.3:
        reasons.append('ESã®å†…å®¹ãŒé¡ä¼¼')
    if industry_match == 1.0:
        reasons.append('å¿—æœ›æ¥­ç•Œã¨ä¸€è‡´')
    if university_match == 1.0:
        reasons.append('åŒã˜å¤§å­¦ã‹ã‚‰ã®æ¡ç”¨å®Ÿç¸¾')
    if len(company_data) >= 10:
        reasons.append(f'{len(company_data)}ä»¶ã®åˆæ ¼ESå®Ÿç¸¾ã‚ã‚Š')
    elif len(company_data) >= 5:
        reasons.append(f'{len(company_data)}ä»¶ã®ESå®Ÿç¸¾ã‚ã‚Š')

    reason = 'ã€'.join(reasons) if reasons else 'ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒãƒ³ã‚°'

    return {
        'name': company_name,
        'industry': str(representative['industry']) if not pd.isna(representative['industry']) else 'ä¸æ˜',
        'matchScore': match_score,
        'reason': reason,
        'dataCount': len(company_data)
    }

def analyze_industry(industry):
    """æ¥­ç•Œåˆ†æ"""
    industry_data = es_data[es_data['industry'].str.contains(industry, na=False)]

    if len(industry_data) == 0:
        return {
            'passRate': 70,
            'avgApplicants': 150,
            'competition': 'ä¸­',
            'recommendations': ['æ¥­ç•Œç ”ç©¶ã‚’æ·±ã‚ã‚‹', 'ä¼æ¥­ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹']
        }

    pass_rate = 75
    avg_applicants = len(industry_data) * 3

    if avg_applicants > 200:
        competition = 'éå¸¸ã«é«˜'
    elif avg_applicants > 150:
        competition = 'é«˜'
    elif avg_applicants > 100:
        competition = 'ä¸­'
    else:
        competition = 'ä½'

    recommendations_map = {
        'IT': ['æŠ€è¡“ã‚¹ã‚­ãƒ«ã®è¨¼æ˜', 'ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®ä½œæˆ', 'æœ€æ–°æŠ€è¡“ã®ã‚­ãƒ£ãƒƒãƒã‚¢ãƒƒãƒ—'],
        'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°': ['ã‚±ãƒ¼ã‚¹é¢æ¥å¯¾ç­–', 'è«–ç†çš„æ€è€ƒåŠ›ã®å¼·åŒ–', 'ãƒ•ã‚§ãƒ«ãƒŸæ¨å®šã®ç·´ç¿’'],
        'é‡‘è': ['é‡‘èçŸ¥è­˜ã®ç¿’å¾—', 'æ•°å­—ã«å¼·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰', 'èª å®Ÿã•ã®ã‚¢ãƒ”ãƒ¼ãƒ«'],
        'ãƒ¡ãƒ¼ã‚«ãƒ¼': ['æŠ€è¡“åŠ›ã®è¨¼æ˜', 'é•·æœŸçš„ãªã‚­ãƒ£ãƒªã‚¢ãƒ“ã‚¸ãƒ§ãƒ³', 'ã‚‚ã®ã¥ãã‚Šã¸ã®æƒ…ç†±']
    }

    recommendations = recommendations_map.get(
        industry,
        ['æ¥­ç•Œç ”ç©¶ã‚’æ·±ã‚ã‚‹', 'ä¼æ¥­ã®ç‰¹å¾´ã‚’ç†è§£ã™ã‚‹', 'è‡ªå·±åˆ†æã‚’å¾¹åº•ã™ã‚‹']
    )

    return {
        'passRate': pass_rate,
        'avgApplicants': avg_applicants,
        'competition': competition,
        'recommendations': recommendations
    }

def analyze_gakuchika(gakuchika_text):
    """ã‚¬ã‚¯ãƒã‚«åˆ†æ"""
    strengths = []
    improvements = []

    if any(word in gakuchika_text for word in ['æ•°å€¤', 'çµæœ', 'æˆæœ', '%', 'äºº']):
        strengths.append('å…·ä½“çš„ãªæ•°å€¤ãƒ»æˆæœã®è¨˜è¼‰')

    if any(word in gakuchika_text for word in ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„']):
        strengths.append('èª²é¡Œè§£æ±ºã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜ç¢º')

    if len(gakuchika_text) >= 300:
        strengths.append('ååˆ†ãªåˆ†é‡ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹')

    if not any(word in gakuchika_text for word in ['ãƒãƒ¼ãƒ ', 'å”åŠ›', 'é€£æº', 'ãƒ¡ãƒ³ãƒãƒ¼']):
        improvements.append('ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¦ç´ ã‚’è¿½åŠ ')

    if not any(word in gakuchika_text for word in ['å­¦ã‚“ã ', 'å¾—ãŸ', 'æˆé•·']):
        improvements.append('å­¦ã³ã‚„æˆé•·ã®è¦ç´ ã‚’å¼·èª¿')

    if len(gakuchika_text) < 200:
        improvements.append('ã‚‚ã†å°‘ã—è©³ã—ãè¨˜è¿°ã™ã‚‹')

    return {
        'strengths': strengths if strengths else ['åŸºæœ¬çš„ãªæ§‹æˆã¯è‰¯å¥½'],
        'improvements': improvements if improvements else ['ç¾çŠ¶ã§è‰¯ã„å†…å®¹ã§ã™']
    }

def analyze_es_answers(answers):
    """ESåˆ†æï¼ˆè¤‡æ•°å›ç­”å¯¾å¿œï¼‰"""
    all_text = ' '.join([a for a in answers if a])

    strengths = []
    improvements = []

    if any(word in all_text for word in ['æ•°å€¤', 'çµæœ', 'æˆæœ', '%', 'äºº', 'ä»¶', 'å€']):
        strengths.append('å…·ä½“çš„ãªæ•°å€¤ãƒ»æˆæœã®è¨˜è¼‰')
    if any(word in all_text for word in ['èª²é¡Œ', 'å•é¡Œ', 'è§£æ±º', 'æ”¹å–„', 'å…‹æœ']):
        strengths.append('èª²é¡Œè§£æ±ºã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ˜ç¢º')
    if any(word in all_text for word in ['ãƒãƒ¼ãƒ ', 'å”åŠ›', 'é€£æº', 'ãƒ¡ãƒ³ãƒãƒ¼', 'çµ„ç¹”']):
        strengths.append('ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¦ç´ ãŒã‚ã‚‹')
    if len(all_text) >= 500:
        strengths.append('ååˆ†ãªåˆ†é‡ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹')

    if not any(word in all_text for word in ['å­¦ã‚“ã ', 'å¾—ãŸ', 'æˆé•·', 'çµŒé¨“']):
        improvements.append('å­¦ã³ã‚„æˆé•·ã®è¦ç´ ã‚’å¼·èª¿')
    if not any(word in all_text for word in ['å…·ä½“çš„', 'ä¾‹ãˆã°', 'å®Ÿéš›ã«']):
        improvements.append('ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿½åŠ ')
    if len(all_text) < 300:
        improvements.append('ã‚‚ã†å°‘ã—è©³ã—ãè¨˜è¿°ã™ã‚‹')

    return {
        'strengths': strengths if strengths else ['åŸºæœ¬çš„ãªæ§‹æˆã¯è‰¯å¥½'],
        'improvements': improvements if improvements else ['ç¾çŠ¶ã§è‰¯ã„å†…å®¹ã§ã™']
    }

def get_similar_es_samples(similar_es, top_n=3):
    """é¡ä¼¼ESã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
    samples = []

    for idx, row in similar_es.head(top_n).iterrows():
        user_info = str(row.get('user_info', ''))

        # å’æ¥­å¹´åº¦ã‚’æŠ½å‡º
        grad_year_match = re.search(r'(\d{2})å’', user_info)
        grad_year = grad_year_match.group(1) + 'å’' if grad_year_match else 'ä¸æ˜'

        university = row.get('university', 'ä¸æ˜')

        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚’æŠ½å‡º
        major_match = re.search(r'\|\s*([^|]+)\s*\|', user_info)
        major = major_match.group(1).strip() if major_match else 'ä¸æ˜'

        es_content = []
        for i in range(1, 4):
            question = row.get(f'question_{i}', '')
            answer = row.get(f'answer_{i}', '')

            if question and answer and str(question).strip() and str(answer).strip():
                es_content.append({
                    'question': str(question).strip(),
                    'answer': str(answer).strip()[:500] + ('...' if len(str(answer)) > 500 else '')
                })

        if len(es_content) > 0:
            sample = {
                'company': str(row['company_name']),
                'industry': str(row['industry']) if not pd.isna(row['industry']) else 'ä¸æ˜',
                'result': str(row['result_status']),
                'similarity': round(float(row['similarity_score']) * 100, 1),
                'profile': {
                    'university': university,
                    'major': major,
                    'gradYear': grad_year
                },
                'esContent': es_content
            }
            samples.append(sample)

    return samples

# ============================================
# HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆç¾ã—ã„UIï¼‰
# ============================================

# HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ templates/index.html ã‹ã‚‰èª­ã¿è¾¼ã¿

# ============================================
# Flaskãƒ«ãƒ¼ãƒˆ
# ============================================

@app.route('/')
def home():
    """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰UI - ãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã‚“ã HTMLã‚’è¿”ã™"""
    print("\nğŸŒ ãƒšãƒ¼ã‚¸ç”Ÿæˆä¸­...")

    # é¸æŠè‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    embedded_data = {
        'universities': universities_list[:200],  # æœ€åˆã®200æ ¡
        'industries': industries_list,
        'companies': companies_list[:300],  # æœ€åˆã®300ç¤¾
        'commonQuestions': common_questions,
        'companyCounts': {k: v for k, v in company_counts.items() if k in companies_list[:300]},
        'industryCounts': industry_counts
    }

    # JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºï¼ˆensure_ascii=Trueã§å®‰å…¨ã«ï¼‰
    embedded_data_json = json.dumps(embedded_data, ensure_ascii=True)

    # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template_path = os.path.join(template_dir, 'index.html')
    with open(template_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    # <script type="application/json">ã§åŸ‹ã‚è¾¼ã¿
    embedded_script = f"""
    <script id="embedded-data" type="application/json">
{embedded_data_json}
    </script>
    """

    # HTMLã®</head>ç›´å‰ã«æŒ¿å…¥
    html_content = html_content.replace('</head>', embedded_script + '\n</head>')

    print(f"  âœ… ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿å®Œäº†: å¤§å­¦{len(embedded_data['universities'])}æ ¡, æ¥­ç•Œ{len(embedded_data['industries'])}ç¨®é¡")

    return Response(html_content, mimetype='text/html')

@app.route('/analyze', methods=['POST'])
def analyze_es():
    """ESè¨ºæ–­API - è¤‡æ•°ESè³ªå•å¯¾å¿œ"""
    try:
        data = request.get_json()

        # è¤‡æ•°ã®ESå›ç­”ã«å¯¾å¿œ
        if not data.get('esAnswers') or len(data.get('esAnswers', [])) == 0:
            return jsonify({'error': 'ESå›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„'}), 400

        has_long_answer = any(len(ans) >= 100 for ans in data['esAnswers'])
        if not has_long_answer:
            return jsonify({'error': 'å°‘ãªãã¨ã‚‚1ã¤ã®å›ç­”ã¯100æ–‡å­—ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„'}), 400

        if not data.get('targetIndustry'):
            return jsonify({'error': 'å¿—æœ›æ¥­ç•Œã‚’é¸æŠã—ã¦ãã ã•ã„'}), 400

        # å…¨ã¦ã®å›ç­”ã‚’çµåˆã—ã¦é¡ä¼¼åº¦è¨ˆç®—
        combined_answers = ' '.join(data['esAnswers'])
        similar_es = calculate_similarity(combined_answers, top_n=100)

        top_companies = get_top_companies(
            similar_es,
            data['targetIndustry'],
            data.get('university', ''),
            top_n=5
        )

        industry_analysis = analyze_industry(data['targetIndustry'])
        es_analysis = analyze_es_answers(data['esAnswers'])
        similar_es_samples = get_similar_es_samples(similar_es, top_n=3)

        # å¿—æœ›ä¼æ¥­ã®ãƒãƒƒãƒç‡ã‚’è¨ˆç®—ï¼ˆç¬¬ä¸‰å¿—æœ›ã¾ã§ï¼‰
        target_companies_match = []
        if data.get('targetCompanies') and len(data['targetCompanies']) > 0:
            for i, target_company in enumerate(data['targetCompanies'], 1):
                if target_company and target_company.strip():
                    match_result = calculate_target_company_match(
                        target_company,
                        similar_es,
                        data['targetIndustry'],
                        data.get('university', ''),
                        rank=i  # å¿—æœ›é †ä½ã‚’æ¸¡ã™
                    )
                    if match_result:
                        # å¿—æœ›é †ä½ã‚’è¿½åŠ 
                        match_result['rank'] = i
                        target_companies_match.append(match_result)

        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_es_count = len(es_data)
        matched_es_count = len(similar_es)
        industry_es_count = len(es_data[es_data['industry'].str.contains(data['targetIndustry'], na=False)])

        # å¿—æœ›ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        target_companies_data_count = {}
        if data.get('targetCompanies') and len(data['targetCompanies']) > 0:
            for target_company in data['targetCompanies']:
                if target_company and target_company.strip():
                    count = len(es_data[es_data['company_name'] == target_company])
                    target_companies_data_count[target_company] = count

        # ç¬¬ä¸‰å¿—æœ›ã¾ã§ã®ãƒãƒƒãƒç‡ã®å¹³å‡ã‚’è¨ˆç®—
        avg_match_rate = 0
        if len(target_companies_match) > 0:
            avg_match_rate = sum(item['matchScore'] for item in target_companies_match) / len(target_companies_match)

        response = {
            'matchCompanies': top_companies,
            'industryAnalysis': industry_analysis,
            'esAnalysis': es_analysis,
            'similarESSamples': similar_es_samples,
            'targetCompaniesMatch': target_companies_match,  # ç¬¬ä¸‰å¿—æœ›ã¾ã§ã®ãƒãƒƒãƒç‡
            'dataStatistics': {
                'totalEsCount': total_es_count,
                'matchedEsCount': matched_es_count,
                'industryEsCount': industry_es_count,
                'targetCompaniesDataCount': target_companies_data_count,
                'avgMatchRate': round(avg_match_rate, 1)
            },
            'userInfo': {
                'university': data.get('university'),
                'major': data.get('major'),
                'graduationYear': data.get('graduationYear')
            }
        }

        return jsonify(response)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
